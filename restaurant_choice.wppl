// set our target aspitation and safety bound lambdas:

var aleph0 = 1.5, // desired expected return
    maxLambda = 0.95, // upper bound on local relative aspiration in each step (must be minLambda...1)
    minLambda = 0.05; // lower bound on local relative aspiration in each step (must be 0...maxLambda)


// fold: Restaurant constants, tableToUtilityFunction

var ___ = ' ';
var DN = { name : 'Donut N' };
var DS = { name : 'Donut S' };
var V = { name : 'Veg' };
var N = { name : 'Noodle' };

var tableToUtilityFunction = function(table, feature) {
  return function(state, action) {
    var stateFeatureName = feature(state).name;
    return stateFeatureName ? table[stateFeatureName] : table.timeCost;
  };
};
// 

// Construct world 

var grid = [
  ['#', '#', '#', '#',  V , '#'],
  ['#', '#', '#', ___, ___, ___],
  ['#', '#', DN , ___, '#', ___],
  ['#', '#', '#', ___, '#', ___],
  ['#', '#', '#', ___, ___, ___],
  ['#', '#', '#', ___, '#',  N ],
  [___, ___, ___, ___, '#', '#'],
  [DS , '#', '#', ___, '#', '#']
];

var mdp = makeGridWorldMDP({
  grid,
  start: [3, 1],
  totalTime: 9
});

var world = mdp.world;
var transition = world.transition;
var stateToActions = world.stateToActions;


// Construct utility function

var utilityTable = {
  'Donut S': 1,
  'Donut N': 1,
  'Veg': 3,
  'Noodle': 2,
  'timeCost': -0.1
};

var indicatorIncrement = tableToUtilityFunction(utilityTable, world.feature);


// Auxiliary functions:

var interpolate = function(x, lam, y) {
  // this is denoted  x : lam : y  in formulas
  return x + lam * (y - x);
}
var relativePosition = function(x, z, y) {
  // this is denoted  x \ z \ y  in formulas
  return y != x ? (z - x) / (y - x) : 0.5;
}
var clip = function(x, z, y) {
  // this is denoted  x[ z ]y  in formulas
  return Math.min(Math.max(x, z), y);
}

// Construct agent

var makeAgent = function(indicatorIncrement) {
  // Note: I renamed some things so that they no longer sound as if larger is better:
  // reward -> r
  // utility -> indicatorIncrement

  var act = dp.cache(function(state, aleph){
    // Calculate expected return in state, given aspiration aleph:
    var v = V(state, aleph);
    // Now we find the two Q table entries, qHi and qLo, that are closest to v from above and below: 
    var actions = stateToActions(state),
        Qstate = function(a) { return Q(state, a, aleph); },
        qHi = minWith(function(a) {return Qstate(a) >= v ? Qstate(a) : 1e10}, actions)[1],
        qLo = maxWith(function(a) {return Qstate(a) <= v ? Qstate(a) : -1e10}, actions)[1];
    // From those a that meet these values, choose the ones that minimize the variance of return
    // by minimizing the squared deviation from expected return (which equals v):
    var squaredDevFromVHi = function(a) { 
          var q2 = Q2(state, a, aleph),
              variance = q2 - Math.pow(qHi,2);
          return variance + Math.pow(qHi - v,2);
        },
        squaredDevFromVLo = function(a) {
          var q2 = Q2(state, a, aleph),
              variance = q2 - Math.pow(qLo,2);
          return variance + Math.pow(qLo - v,2);
        },
        aHi = minWith(squaredDevFromVHi, filter(function(a) {return Qstate(a) == qHi}, actions))[0],
        aLo = minWith(squaredDevFromVLo, filter(function(a) {return Qstate(a) == qLo}, actions))[0];
    // Before, we simply chose two arbitrary actions that achieve these values:
    // var aHi = maxWith(function(a) {return Qstate(a) == qHi ? 1 : 0}, actions)[0],
    //    aLo = maxWith(function(a) {return Qstate(a) == qLo ? 1 : 0}, actions)[0];
    // Our local policy that guarantees q in expectation is a suitable mixture of these two actions:
    var p = qHi > qLo ? (v - qLo) / (qHi - qLo) : 0.5,
        localPolicy = Categorical({ps: [p, 1-p], vs: [aHi, aLo]});
    return localPolicy;
  });

  // Note: we do no longer need the former function expectedUtility, so I removed it.

  // Compute the upper and lower bounds for Q and V that can be achieved:
  // (TODO: generalize to use maxLambda, minLambda)

  // Compute the Q and V functions of the classical maximization problem (if maxLambda==1)
  // or of the LRA-based problem (if maxLambda<1):
  var maxFeasibleQ = dp.cache(function(state, action){
    var r = indicatorIncrement(state, action);
    if (state.terminateAfterAction){
      return r;
    } else {
      return r + expectation(Infer({ model() { return maxFeasibleV(transition(state, action)); }}));
    }
  });
  var maxFeasibleV = dp.cache(function(state){
    var actions = stateToActions(state),
        Qstate = function(a) { return maxFeasibleQ(state, a); };
    if (maxLambda == 1) {
      return maxWith(Qstate, actions)[1];
    } else {
      var v = interpolate(minWith(Qstate, actions)[1], maxLambda, maxWith(Qstate, actions)[1]);
      return v;
    }
  });

  // Compute the Q and V functions of the corresponding minimization (!) problem (if minLambda==0)
  // or of the LRA-based problem (if minLambda>0):
  var minFeasibleQ = dp.cache(function(state, action){
    var r = indicatorIncrement(state, action);
    if (state.terminateAfterAction){
      return r;
    } else {
      return r + expectation(Infer({ model() { return minFeasibleV(transition(state, action)); }}));
    }
  });
  var minFeasibleV = dp.cache(function(state){
    var actions = stateToActions(state),
        Qstate = function(a) { return minFeasibleQ(state, a); };
    if (minLambda == 0) {
      return minWith(Qstate, actions)[1];
    } else {
      var v = interpolate(minWith(Qstate, actions)[1], minLambda, maxWith(Qstate, actions)[1]);
      return v;
    }
  });

  // When using action in state, we can get any expected return in the interval
  // [minFeasibleQ(state, action), maxFeasibleQ(state, action)].
  // So when having aspiration aleph, we can still fulfill it in expectation if it lies in the interval.
  // Therefore, when using action in state at aspiration aleph, we expect to get an expected return
  // that equals aleph clipped to that interval:
  var Q = dp.cache(function(state, action, aleph) {
    return clip(minFeasibleQ(state, action), aleph, maxFeasibleQ(state, action));
  });
  // When in state, we can get any expected return in the interval
  // [minFeasibleV(state), maxFeasibleV(state)].
  // So when having aspiration aleph, we can still fulfill it in expectation if it lies in the interval.
  // Therefore, when in state at aspiration aleph, we expect to get an expected return
  // that equals aleph clipped to that interval:
  var V = dp.cache(function(state, aleph) {
    return clip(minFeasibleV(state), aleph, maxFeasibleV(state));
  });

  // Expected squared return, for computing the variance of return:
  var Q2 = dp.cache(function(state, action, aleph) {
    return expectation(Infer({ model() {
      var r = indicatorIncrement(state, action);
      if (state.terminateAfterAction){
        return Math.pow(r,2);
      } else {
        var nextState = transition(state, action),
            nextAleph = propagateAspiration(state, aleph, action, r, nextState);
        return Math.pow(r,2) + 2*r*V(nextState, nextAleph) + V2(nextState, nextAleph);
      }
    }}));
  });
  var V2 = dp.cache(function(state, aleph) {
    return expectation(Infer({ model() { return Q2(state, sample(act(state, aleph)), aleph); }}));
  });

  var propagateAspiration = dp.cache(function(state, aleph, action, r, nextState) {

    // recover the adjusted target value that we had after choosing action:
    // (Note: We have E(q | action ~ localPolicy) = v) by construction of localPolicy in act().)
    var q = Q(state, action, aleph);

    // compute the relative position of that target in the expectation that we had of 
    //    r plus next feasibility interval 
    // before we knew which state we would land in:
    var lam = relativePosition(minFeasibleQ(state, action), q, maxFeasibleQ(state, action)); 
    // (this is between 0 and 1 by definition of q and Q.)

    // rescale the target to the feasibility interval of the state that we landed in:
    var nextAleph = interpolate(minFeasibleV(nextState), lam, maxFeasibleV(nextState));

    return nextAleph;
  });

  return { act: act, propagateAspiration: propagateAspiration };
};

var agent = makeAgent(indicatorIncrement);
var act = agent.act;
var propagate_aspiration = agent.propagateAspiration;

// Generate and draw a trajectory


var simulate = function(state, aleph) {
  var localPolicy = act(state, aleph);
  var action = sample(localPolicy);
  var r = indicatorIncrement(state, action);
  var nextState = transition(state, action);
  var nextAleph = propagate_aspiration(state, aleph, action, r, nextState);
  var sa = [state, action];
  if (state.terminateAfterAction) {
    return { traj: [sa], return: r };
  } else {
    var nextOut = simulate(nextState, nextAleph);
    return { traj: [sa].concat(nextOut.traj), return: r + nextOut.return };
  }
};


// calculate expected return and return std.dev. by using the inbuilt expectation function:

var expectedReturn = expectation(Infer({ model() {
  return simulate(mdp.startState, aleph0).return;
}}));
console.log("in expectation, we expected return", aleph0, "and actually get", expectedReturn);

var expectedSquaredReturn = expectation(Infer({ model() {
  return Math.pow(simulate(mdp.startState, aleph0).return,2);
}})),
stddev = Math.sqrt(expectedSquaredReturn - Math.pow(expectedReturn,2));
console.log("return had a std.dev. of", stddev);


// simulate a single trajectory:

var out = simulate(mdp.startState, aleph0);
var trajectory = out.traj;
viz.gridworld(world, { trajectory: map(first, trajectory) })

