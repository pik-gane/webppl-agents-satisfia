// make sure code runs in browser and on command line:
var inBrowser = typeof(argv) === 'undefined',
    _argv = inBrowser ? {} : argv;

// agent parameters:
var params = extend({
    maxLambda: 0.99, // upper bound on local relative aspiration in each step (must be minLambda...1)
    minLambda: 0.01, // lower bound on local relative aspiration in each step (must be 0...maxLambda)
    lossCoeff4variance: 100.0, // weight of variance in loss function, must be >= 0
    lossCoeff4KLdiv: 0.0, // weight of KL divergence in loss function, must be >= 0
    lossCoeff4entropy: 1.0, // weight of entropy in loss function, must be >= 0
    lossCoeff4random: 0.0, // weight of random noise in loss function, must be >= 0
    onlyUseClosestActions: true, // if true, only use the two actions that are closest to the target aspiration in expectation
    softmaxInvTemp: 1.0, // inverse temperature of softmax mixture of actions, must be > 0
    rescalingDegree: 1.0, // degree (0...1) of aspiration rescaling. (expectation is only preserved if this is 1.0)
  }, _argv);

// simulation options:
var options = extend({
      aleph0: [1.2,1.4], // initial global aspiration (desired expected return);
      debug: false, // if true, print debug messages
    }, _argv),
    aleph0 = typeof(options.aleph0Lo) === 'undefined' ? options.aleph0 : [options.aleph0Lo, options.aleph0Hi];

// fold: Restaurant constants, tableToUtilityFunction

var ___ = ' ';
var G1 = { name : 'G1' };
var G3 = { name : 'G3' };
var G4 = { name : 'G4' };

var tableToIndicatorIncrementFct = function(table, feature) {
  return function(state, action) {
    var f = feature(state), stateFeatureName = f.name, inc = stateFeatureName ? table[stateFeatureName] : table[f['0']];
    return inc;
  };
};
// 

// Construct world 



// Grid 1: Aspiration is 2 here. The desired solution is to go north. Always going left would give too little,
// always right too much. A mixture would give enough in expectation, but is not desired solution. Agent starts at [2,1]

// var grid = [
// ["#", "#",  "#",  "#",  "#"],
// ["#",  "#",  G2, "#", "#"],
// ["#", G1, ___,  G3, "#"],
// ["#",  "#",  "#",   "#",   "#"]
// ];


// Grid 2: Aspiration is 2 here again. The desired solution is to go to G2, albeit it would require more steps. 
// The starting position here is [1,2]
// var grid = [
//["#",   "#",  "#",   "#",  "#"],
//["#",   ____ ,  ___, ___,  "#"],
//["#",  G1,  G3,  G2,  "#"], 
//["#",  "#",   "#",   "#",   "#"]
//];

// Grid 3: Aspiration is 2. Desired is to go the the Goal state that gives 2 instead of flipping 
// a coin and going to one of the other two goal states with positive return. The starting position is [2,2]
// var grid = [
//["#",  "#",  "#",  "#",  "#"],
//["#",  G0, "#", G3, "#"],
//["#", ___,  ___, ___,  "#"],
//["#",  G1,  "#",  G2,  "#"],
//["#",  "#",  "#",  "#",  "#"]
//];

// Grid 4: Aspiration is 1.5. The aspiration is that the agent flips a coint between going to G1 and G2 to arrive at the
// aspiration of 1.5 (leading to smaller variance) than mixing between 0 and 3.
// var grid = [
// ["#",  "#",  "#",  "#",  "#", "#"],
// ["#", ___, ___,  ___, ___,   "#"],
// ["#",  G1,  G3,  G2,  G0,  "#"]
// ["#",  "#",  "#",  "#",   "#", "#"]
// ];

// Grid 6: Aleph 0 is 2 here. The desired behaviour is to go through d to G4 instead of going left and flipping a coin.
var grid = [
["#",  "#",  "#",  "#",  "#"],
["#",  G1, "#", G4, "#"],
["#", ___,  ___,  "d",  "#"],
["#",  G3,  "#",  "#",  "#"],
["#",  "#",  "#",  "#",  "#"]
];


var mdp = makeGridWorldMDP({
  grid,
  start: [2, 2],
  totalTime: 3
});

var world = mdp.world;
var transition = world.transition;
var stateToActions = world.stateToActions;


// Specify distribution of indicator increments (via expected value and variancs) 
// (Note: under a "maximization" paradigm, this would be called "reward" or "utility")

var expectedIndicatorIncrementTable = {
  'G1': 1,
  'G3': 3,
  'G4': 4,
  'd': -2,
  ' ': 0
};
var varianceOfIndicatorIncrementTable =  {
  'G1': 0,
  'G3': 0,
  'G4': 0,
  'd': 0,
  ' ': 0
};


var f0 = world.feature, feature = function(s) {return extend(f0(s), {test: "hello"})};

var expectedIndicatorIncrement = tableToIndicatorIncrementFct(expectedIndicatorIncrementTable, feature),
    varianceOfIndicatorIncrement = tableToIndicatorIncrementFct(varianceOfIndicatorIncrementTable, feature); 


// uninformedP policy for use in Shannon entropy: 
var uninformedPolicy = function(state) {
  return Categorical({vs: ["u", "d", "l", "r"], ps: [0.25, 0.25, 0.25, 0.25]});
}
// (Note: when refining an action a into variants a', a'', 
// the probabilities of a under uninformedPolicy should split into two parts for a' and a'' additively,
// since then behavior remains consistent. In particular, if an action is cloned and its uninformedPolicy 
// probability is split somehow, behavior should remain invariant.)

// Reference policy for use in KL divergence
// (can be used to steer agent towards certain actions):
var refPolicy = function(state) { 
  return Categorical({vs: ["u", "d", "l", "r"], ps: [0.7, 0.1, 0.1, 0.1]}); 
};

// initialize the agent
var agent = makeMDPAgentSatisfia(extend(params, {
    expectedIndicatorIncrement, varianceOfIndicatorIncrement, 
    uninformedPolicy, refPolicy,
    options
  }), world);

// extract its methods:
var getLocalPolicy = agent.getLocalPolicy, propagateAspiration = agent.propagateAspiration,
    getAspiration4state = agent.getAspiration4state, 
    V = agent.V, V2 = agent.V2,
    entropy = agent.entropy, KLdiv = agent.KLdiv;

// Generate and draw a trajectory:
var simulate = function(state, aleph) {
  if (options.debug) console.log("simulate", state, aleph);
  var localPolicy = getLocalPolicy(state, aleph),
      action = sample(localPolicy),
      r = expectedIndicatorIncrement(state, action),
      r2 = squared(r) + varianceOfIndicatorIncrement(state, action);
  var sa = [state, action];
console.log(localPolicy);
  if (state.terminateAfterAction) {
    return { 
      trajectory: [sa], // sequence of [state, action] pairs
      conditionalExpectedIndicator: r, // expected indicator conditional on this trajectory
      conditionalExpectedSquaredIndicator: r2 // expected squared indicator conditional on this trajectory
    };
  } else {
    var nextState = transition(state, action),
        nextAleph = propagateAspiration(state, aleph, action, r, nextState),
        nextOut = simulate(nextState, nextAleph);
    return { 
      trajectory: [sa].concat(nextOut.trajectory), 
      conditionalExpectedIndicator: r + nextOut.conditionalExpectedIndicator,
      conditionalExpectedSquaredIndicator: r2 + 2*r*nextOut.conditionalExpectedIndicator + nextOut.conditionalExpectedSquaredIndicator
    };
  }
};


// calculate expected return and return std.dev. by using the inbuilt expectation function:

var expectedIndicator = expectation(Infer({ model() {
  return simulate(mdp.startState, aleph0).conditionalExpectedIndicator;
}}));
console.log("in expectation, we desired indicator", aleph0, "and actually get", expectedIndicator);

var expectedSquaredIndicator = expectation(Infer({ model() {
  return simulate(mdp.startState, aleph0).conditionalExpectedSquaredIndicator;
}})),
stddev = Math.sqrt(expectedSquaredIndicator - Math.pow(expectedIndicator,2));
console.log("indicator has a std.dev. of", stddev, 
            "which should equal", Math.sqrt(V2(mdp.startState, aleph0) - Math.pow(V(mdp.startState, aleph0),2)));

console.log("Entropy of policy is", entropy(mdp.startState, aleph0));
console.log("The max lambda is", params.maxLambda)
console.log("The min lambda is", params.minLambda)

console.log("KL divergence from reference policy is", KLdiv(mdp.startState, aleph0));
if (inBrowser) {
  // simulate and show a single trajectory:
  var out = simulate(mdp.startState, aleph0);
  viz.gridworld(world, { trajectory: map(first, out.trajectory) })
}




// Define the filename for the JSON file
var filename = 'Grid6.json';

// Read existing JSON data from the file (if it exists)
var existingData = json.read(filename) || [];

// Assuming you have a single data object with keys and values
var data1 = {
  key1: 'expectedIndicator',
  value1: expectedIndicator,
  key2: 'aleph0',
  value2: aleph0,
  key3: 'minlambda',
  value3: params.minLambda,
  key4: 'maxLambda',
  value4: params.maxLambda,
  key5: 'Standard deviation',
  value5: stddev

};

// Add new data to the existing data
existingData.push(data1);

// Write the updated data back to the JSON file
json.write(filename, existingData);
