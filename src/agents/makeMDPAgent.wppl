var hasProperties = function(object, listProperties) {
  assert.ok(_.isObject(object) && _.isArray(listProperties), 'fail hasProperties');
  return _.every(map(
    function(property) {
      return _.has(object, property);
    }, listProperties));
};


var makeMDPAgentOptimal = function(params, world) {
  // *params* should be an object containing *utility*, a utility function, and
  // *alpha*, which regulates the agent's softmax noise.
  map(function(s) {
    assert.ok(params.hasOwnProperty(s), 'makeMDPAgent args');
  }, ['utility', 'alpha']);

  var stateToActions = world.stateToActions;
  var transition = world.transition;
  var utility = params.utility;
  var alpha = params.alpha;

  var act = dp.cache(
    function(state) {
      return Infer({
        method: 'enumerate'
      }, function() {
        var action = uniformDraw(stateToActions(state));
        var eu = expectedUtility(state, action);
        factor(alpha * eu);
        return action;
      });
    });

  var expectedUtility = dp.cache(
    function(state, action) {
      var u = utility(state, action);
      if (state.terminateAfterAction) {
        return u;
      } else {
        return u + expectation(Infer({
          method: 'enumerate'
        }, function() {
          var nextState = transition(state, action);
          var nextAction = sample(act(nextState));
          return expectedUtility(nextState, nextAction);
        }));
      }
    });

  return {
    params,
    expectedUtility,
    act
  };
};


var makeMDPAgentHyperbolic = function(params, world) {
  assert.ok(hasProperties(params, ['utility', 'alpha', 'discount', 'sophisticatedOrNaive']),
            'makeMDPAgentHyperbolic params');

  var stateToActions = world.stateToActions;
  var transition = world.transition;
  var utility = params.utility;

  // we can specify a discount function so that our 'hyperbolic' agent can
  // actually be an exponential discounter (or some other kind of discounter)
  var paramsDiscountFunction = params.discountFunction;

  var discountFunction = (
    paramsDiscountFunction ||
    function(delay) {
      return 1 / (1 + params.discount * delay);
    });

  var isNaive = params.sophisticatedOrNaive == 'naive';

  var act = dp.cache(
    function(state, delay) {
      var delay = delay ? delay : 0; //make sure delay is never 'undefined'

      return Infer({
        method: 'enumerate'
      }, function() {
        var action = uniformDraw(stateToActions(state));
        var eu = expectedUtility(state, action, delay);
        factor(params.alpha * eu);
        return action;
      });
    });

  var expectedUtility = dp.cache(
    function(state, action, delay) {
      var u = discountFunction(delay) * utility(state, action);
      assert.ok(!_.isUndefined(u),
        "utility undefined" + JSON.stringify([state, action, delay, utility(state, action)]));

      if (state.terminateAfterAction) {
        return u;
      } else {
        return u + expectation(Infer({
          method: 'enumerate'
        }, function() {
          var nextState = transition(state, action);
          var perceivedDelay = isNaive ? delay + 1 : 0;
          var nextAction = sample(act(nextState, perceivedDelay));
          return expectedUtility(nextState, nextAction, delay + 1);
        }));
      }
    });

  return {
    params,
    expectedUtility,
    act
  };
};


var isOptimalMDPAgent = function(agentParams) {
  var optimalProperties = function() {
    return !(_.has(agentParams, 'discount') ||
      _.has(agentParams, 'discountFunction') ||
      _.has(agentParams, 'sophisticatedOrNaive'));
  };
  return _.isUndefined(agentParams.optimal) ? optimalProperties() : agentParams.optimal;
};


var makeMDPAgent = function(params, world) {
  return (isOptimalMDPAgent(params) ?
          makeMDPAgentOptimal(params, world) :
          makeMDPAgentHyperbolic(params, world));
};



// SatisfIA agent:

/* TODO:
- rename parameters to match internal naming
- rename increment to delta and indicator to total
*/

var makeMDPAgentSatisfia = function(params_, world) {

  // extend default parameters and options by supplied ones:
  var params = extend(extend({
    maxLambda: 1, // upper bound on local relative aspiration in each step (must be minLambda...1)
    minLambda: 0, // lower bound on local relative aspiration in each step (must be 0...maxLambda)
    lossCoeff4variance: 1, // weight of variance in loss function, must be >= 0
    lossCoeff4DeltaVariation: 0, // weight of variation of Delta in loss function, must be >= 0
    lossCoeff4LRA: 0, // weight of deviation of LRA from 0.5 in loss function, must be >= 0
    lossCoeff4MP: 0, // weight of messing potential in loss function, must be >= 0
    lossCoeff4entropy: 0, // weight of entropy in loss function, must be >= 0
    lossCoeff4KLdiv: 0, // weight of KL divergence in loss function, must be >= 0
    lossCoeff4feasibilityPower: 0, // weight of power of squared feasibility interval width in loss function, must be >= 0
    lossCoeff4otherLoss: 0, // weight of other loss components specified by otherLossIncrement, must be >= 0
    lossCoeff4random: 1e-10, // weight of random tie breaker in loss function, must be >= 0
    onlyUseClosestActions: true, // if true, only use the two actions that are closest to the target aspiration in expectation
    softmaxInvTemp: 1, // inverse temperature of softmax mixture of actions, must be > 0
    rescalingDegree: 1, // degree (0...1) of aspiration rescaling. (expectation is only preserved if this is 1.0)
  }, params_), { options: extend({
    debug: false
  }, params_.options) });

  // extract parameters, options, and world:
  var stateToActions = world.stateToActions,  // function(state)
      transition = world.transition;  // stochastic function(state, action)
  var expectedDelta = params.expectedIndicatorIncrement,  // function(state, action)
      varianceOfDelta = params.varianceOfIndicatorIncrement,  // function(state, action)
      minLambda = params.minLambda, 
      maxLambda = params.maxLambda,
      uninformedStatePriorScore = params.uninformedStatePriorScore,
      uninformedPolicy = params.uninformedPolicy,  // function(state)
      refPolicy = params.refPolicy,  // function(state)
      internalActionEntropy = params.internalActionEntropy,  // function(state, action)
      internalTransitionEntropy = params.internalTransitionEntropy,  // function(state, action, nextState)
      otherLocalLoss = params.otherLossIncrement,  // function(state, action, aleph, v, q, p)
      lossCoeff4variance = params.lossCoeff4variance,
      lossCoeff4DeltaVariation = params.lossCoeff4DeltaVariation,
      lossCoeff4LRA = params.lossCoeff4LRA,
      lossCoeff4MP = params.lossCoeff4MP,
      lossCoeff4entropy = params.lossCoeff4entropy,
      lossCoeff4KLdiv = params.lossCoeff4KLdiv,
      lossCoeff4feasibilityPower = params.lossCoeff4feasibilityPower,
      lossCoeff4otherLoss = params.lossCoeff4otherLoss,
      lossCoeff4random = params.lossCoeff4random,
      onlyUseClosestActions = params.onlyUseClosestActions,
      softmaxInvTemp = params.softmaxInvTemp,
      rescalingDegree = params.rescalingDegree;
  var options = params.options, 
      debug = options.debug;

  assert.ok(lossCoeff4variance >= 0, "lossCoeff4variance must be >= 0");
  assert.ok(lossCoeff4DeltaVariation >= 0, "lossCoeff4DeltaVariation must be >= 0");
  assert.ok(lossCoeff4LRA >= 0, "lossCoeff4LRA must be >= 0");
  assert.ok(lossCoeff4MP >= 0, "lossCoeff4MP must be >= 0");
  assert.ok(lossCoeff4entropy >= 0, "lossCoeff4entropy must be >= 0");
  assert.ok(lossCoeff4KLdiv >= 0, "lossCoeff4KLdiv must be >= 0");
  assert.ok(lossCoeff4feasibilityPower >= 0, "lossCoeff4feasibilityPower must be >= 0");
  assert.ok(lossCoeff4otherLoss >= 0, "lossCoeff4otherLoss must be >= 0");
  assert.ok(lossCoeff4random >= 0, "lossCoeff4random must be >= 0");
  assert.ok(softmaxInvTemp > 0, "softmaxInvTemp must be > 0");
  assert.ok(0 <= rescalingDegree <= 1, "rescalingDegree must be in 0...1");

  if (debug) console.log("makeMDPAgentSatisfia using parameters", params);

  /*  The dependency/callback graph of the following functions is partially recursive 
      and involves aggregation (MIN, MAX, E) operations as follows:

      simulate
      → localPolicy
        → aspiration4state
          → minFeasibleV, maxFeasibleV
            → MIN(minFeasibleQ), MAX(maxFeasibleQ)
              → E(minFeasibleV), E(maxFeasibleV) (RECURSION)
        → aspiration4action
          → minFeasibleQ, maxFeasibleQ
        → combinedLoss
          → Q, Q2, Q_DeltaSquare, Q_ones
            → propagateAspiration (see below)
            → E(V), E(V2), E(V_DeltaSquare), E(V_ones)
              → localPolicy (RECURSION)
              → E(Q), E(Q2), E(Q_DeltaSquare), E(Q_ones) (RECURSION)
          → otherLoss_action
            → propagateAspiration (see below)
            → E(otherLoss_state)
              → localPolicy (RECURSION)
              → E(otherLoss_action) (RECURSION)
          → similarly for other loss components (RECURSION)
      → expectedDelta, varianceOfDelta, transition
      → propagateAspiration
        → aspiration4state
        → aspiration4action
      → simulate (RECURSION)
  */

  // ! recursive function:
  var localPolicy = function(state, aleph) {
    var policyData = localPolicyData(state, aleph);
    return Categorical({vs: policyData[0], ps: policyData[1]});
  }
  var localPolicyData = dp.cache(function(state, aleph){
    if (debug) console.log(" localPolicy", state, aleph);
    // The following quantities are potentially intervals: aleph, v, Qstate(a)

    // Use aspiration aleph to set the desired expected total for this state before (!) having chosen an action:
    var v = getAspiration4state(state, aleph),
        actions = stateToActions(state),
        Qstate = function(action) { return getAspiration4action(state, action, v); };
    // Meet point v or midpoint of interval v with a mixture of at most two actions:
    var vMid = _.isArray(v) ? (v[0] + v[1]) / 2 : v,
        QstateMid = _.isArray(v) ? function(a) { var q = Qstate(a); return (q[0] + q[1]) / 2; } : Qstate;

    // TODO: make closest/all and softmin/argmin independent options:

    if (onlyUseClosestActions) {
      
      // Find the two QstateMid table entries, qHi and qLo, that are closest to vMid from above and below: 
      var qHi = minWith(function(action) {return QstateMid(action) >= vMid ? QstateMid(action) : 1e10}, actions)[1],
          qLo = maxWith(function(action) {return QstateMid(action) <= vMid ? QstateMid(action) : -1e10}, actions)[1];
      var actionsHi = filter(function(action) {return QstateMid(action) == qHi}, actions),
          actionsLo = filter(function(action) {return QstateMid(action) == qLo}, actions);
      // Our local policy that guarantees v in expectation is a suitable mixture of two actions with those Q values:
      var probHi = relativePosition(qLo, vMid, qHi);
      if (debug) console.log("  qHi, qLo, actionsHi, actionsLo, p", qHi, qLo, actionsHi, actionsLo, probHi);

      // From those actions that meet these values, choose a pair that optimizes 
      // an additional safety criterion encoded in a loss function (see below):
      var lossHi = function(action) { return combinedLoss(state, action, aleph, vMid, qHi, probHi); },
          lossLo = function(action) { return combinedLoss(state, action, aleph, vMid, qLo, 1-probHi); },
          actionHi = minWith(lossHi, actionsHi)[0],
          actionLo = minWith(lossLo, actionsLo)[0];
      return actionHi == actionLo ? [[actionHi], [1]] : [[actionHi, actionLo], [probHi, 1 - probHi]];

    } else {
      
      // Use a softmax mixture of all actions, based on the safety loss.
      // (Note: this will lead to larger entropy and will not (?) behave as consistently under MDP refinements
      // in which an action is resolved into a sequence of sub-actions.)

      // First devide actions into those that are above and below vMid:
      var actionsHi = filter(function(action) {return QstateMid(action) >= vMid}, actions),
          actionsLo = filter(function(action) {return QstateMid(action) <= vMid}, actions);
      // Calculate an approximate relative mixture coefficient p for upper group, to be used for approximating KLdiv:
      var approxQHi = expectation(Infer({ model() { return QstateMid(uniformDraw(actionsHi)); }})),
          approxQLo = expectation(Infer({ model() { return QstateMid(uniformDraw(actionsLo)); }})),
          approxP = clip(1e-10, approxQHi > approxQLo ? (vMid - approxQLo) / (approxQHi - approxQLo) : 0.5, 1-1e-10);
      // Calculate loss for each action:          
      var lossHi = function(a) { return combinedLoss(state, a, aleph, vMid, approxQHi, approxP); },
          lossLo = function(a) { return combinedLoss(state, a, aleph, vMid, approxQLo, 1-approxP); };
      // Construct softmax distributions localPolicyHi, localPolicyLo over actionsHi, actionsLo:
      var defaultPolicy = uninformedPolicy ? uninformedPolicy(state) : undefined,
          probabilityFct = function(a) {
            return Math.exp((defaultPolicy ? defaultPolicy.score(a) : 0) - softmaxInvTemp * lossHi(a))
          },
          localPolicyHi = Categorical({vs: actionsHi, ps: map(probabilityFct, actionsHi)}),
          localPolicyLo = Categorical({vs: actionsLo, ps: map(probabilityFct, actionsLo)}); 
      // Calculate the true qHi, qLo, and p from these distributions:
      var qHi = expectation(localPolicyHi, QstateMid),
          qLo = expectation(localPolicyLo, QstateMid),
          probHi = clip(1e-10, relativePosition(qLo, vMid, qHi), 1-1e-10);
      // Finally, mix the two local policies with the true p:
      var localPolicy = Mixture({ps: [probHi, 1 - probHi], dists: [localPolicyHi, localPolicyLo]});
      // To use the cache efficiently, we return the policy as a pair of arrays:
      var actionsTaken = localPolicy.support(),
          probabilityFct = function(a) { return Math.exp(localPolicy.score(a)) },
          probabilities = map(probabilityFct, actionsTaken);
      return [actionsTaken, probabilities];

    }
  });

  // Compute upper and lower feasibility bounds for Q and V that are allowed in view of maxLambda and minLambda:

  // Compute the Q and V functions of the classical maximization problem (if maxLambda==1)
  // or of the LRA-based problem (if maxLambda<1):
  // ! recursive function:
  var maxFeasibleQ = dp.cache(function(state, action){
    var Edel = expectedDelta(state, action),
        q = state.terminateAfterAction 
            ? Edel 
            : Edel + expectation(Infer({ model() { 
              return maxFeasibleV(transition(state, action));  // ! recursion here ! 
            }}));
    if (debug) console.log("   maxFeasibleQ", state, action, q);
    return q;
  });
  // ! recursive function:
  var maxFeasibleV = dp.cache(function(state){
    var actions = stateToActions(state),
        Qstate = function(a) { return maxFeasibleQ(state, a); },  // ! recursion here !
        v = maxLambda == 1 
            ? maxWith(Qstate, actions)[1] 
            : interpolate(minWith(Qstate, actions)[1], maxLambda, maxWith(Qstate, actions)[1]);
    if (debug) console.log("   maxFeasibleV", state, v);
    return v;
  });

  // Compute the Q and V functions of the corresponding minimization (!) problem (if minLambda==0)
  // or of the LRA-based problem (if minLambda>0):
  // ! recursive function:
  var minFeasibleQ = dp.cache(function(state, action){
    var Edel = expectedDelta(state, action),
        q = state.terminateAfterAction 
            ? Edel 
            : Edel + expectation(Infer({ model() { 
              return minFeasibleV(transition(state, action));  // ! recursion here !
            }}));
    if (debug) console.log("   minFeasibleQ", state, action, q);
    return q;
  });
  // ! recursive function:
  var minFeasibleV = dp.cache(function(state){
    var actions = stateToActions(state),
        Qstate = function(action) { return minFeasibleQ(state, action); },  // ! recursion here !
        v = minLambda == 0 
            ? minWith(Qstate, actions)[1] 
            : interpolate(minWith(Qstate, actions)[1], minLambda, maxWith(Qstate, actions)[1]);
    if (debug) console.log("   minFeasibleV", state, v);
    return v;
  });

  // When using action in state, we can get any expected total in the interval
  // [minFeasibleQ(state, action), maxFeasibleQ(state, action)].
  var getAspiration4action = dp.cache(function(state, action, aleph4state){
    if (debug) console.log("  getAspiration4action", state, action, aleph4state)
    var qLo = minFeasibleQ(state, action),
        qHi = maxFeasibleQ(state, action);
    if (!_.isArray(aleph4state)) {
      // So when having aspiration aleph, we can still fulfill it in expectation if it lies in the interval.
      // Therefore, when using action in state at aspiration aleph, 
      // we set our aspiration when choosing this action to aleph clipped to that interval:
      return clip(qLo, aleph4state, qHi);
    } else {
      // For interval aleph, we choose an interval that guarantees that the resulting mixed interval of all
      // localPolicies that localPolicy() may mix with this action will be within aleph: 
      var alephLo = aleph4state[0], alephHi = aleph4state[1], width = alephHi - alephLo;
      return [Math.max(qLo, Math.min(alephLo, qHi - width)), 
              Math.min(qHi, Math.max(alephHi, qLo + width))];
    }
  });
  // When in state, we can get any expected total in the interval
  // [minFeasibleV(state), maxFeasibleV(state)].
  // So when having aspiration aleph, we can still fulfill it in expectation if it lies in the interval.
  // Therefore, when in state at incoming aspiration aleph, 
  // we adjust our aspiration to aleph clipped to that interval:
  var getAspiration4state = dp.cache(function(state, propagatedAleph){
    var res = clip(minFeasibleV(state), propagatedAleph, maxFeasibleV(state));
    if (debug) console.log("  getAspiration4state", state, propagatedAleph, res);
    return res;
  });

  // Actual Q and V functions of resulting policy (always returning scalars):
  // ! recursive function:
  var Q = dp.cache(function(state, action, aleph){
    if (debug) console.log("  Q", state, action, aleph);
    return expectation(Infer({ model() {
      var Edel = expectedDelta(state, action);
      if (state.terminateAfterAction){
        return Edel;
      } else {
        var nextState = transition(state, action),
            nextAleph = propagateAspiration(state, aleph, action, Edel, nextState);
        return Edel + V(nextState, nextAleph);  // ! recursion here !
      }
    }}));
  });
  // ! recursive function:
  var V = dp.cache(function(state, aleph){
    if (debug) console.log("  V", state, aleph);
    return expectation(Infer({ model() { 
      return Q(state,  // ! recursion here !
               sample(localPolicy(state, aleph)),  // ! recursion here ! 
               aleph);
    }}));
  });

  // Expected squared total, for computing the variance of total:
  // ! recursive function:
  var Q2 = dp.cache(function(state, action, aleph){
    if (debug) console.log("  Q2", state, action, aleph);
    return expectation(Infer({ model() {
      var Edel = expectedDelta(state, action),
          EdelSq = squared(Edel) + varianceOfDelta(state, action);
      if (state.terminateAfterAction){
        return EdelSq;
      } else {
        var nextState = transition(state, action),
            nextAleph = propagateAspiration(state, aleph, action, Edel, nextState);
        return EdelSq + 2*Edel*V(nextState, nextAleph) + V2(nextState, nextAleph); // ! recursion here !
      }
    }}));
  });
  // ! recursive function:
  var V2 = dp.cache(function(state, aleph){
    if (debug) console.log("  V2", state, aleph);
    return expectation(Infer({ model() { 
      return Q2(state,  // ! recursion here !
                sample(localPolicy(state, aleph)),  // ! recursion here !
                aleph); 
    }}));
  });

  // Expected total of squared Deltas, for computing the Delta variation along a trajectory:
  // ! recursive function:
  var Q_DeltaSquare = dp.cache(function(state, action, aleph){
    if (debug) console.log("  Q_DeltaSquare", state, action, aleph);
    return expectation(Infer({ model() {
      var Edel = expectedDelta(state, action),
          EdelSq = squared(Edel) + varianceOfDelta(state, action);
      if (state.terminateAfterAction){
        return EdelSq;
      } else {
        var nextState = transition(state, action),
            nextAleph = propagateAspiration(state, aleph, action, Edel, nextState);
        return EdelSq + V_DeltaSquare(nextState, nextAleph);  // ! recursion here !
      }
    }}));
  });
  // ! recursive function:
  var V_DeltaSquare = dp.cache(function(state, aleph){
    if (debug) console.log("  V_DeltaSquare", state, aleph);
    return expectation(Infer({ model() { 
      return Q_DeltaSquare(state,  // ! recursion here !
                           sample(localPolicy(state, aleph)),  // ! recursion here ! 
                           aleph);
    }}));
  });

  // Expected total of ones (= expected length of trajectory), for computing the Delta variation along a trajectory:
  // ! recursive function:
  var Q_ones = dp.cache(function(state, action, aleph){
    if (debug) console.log("  Q_ones", state, action, aleph);
    return expectation(Infer({ model() {
      var Edel = expectedDelta(state, action);
      if (state.terminateAfterAction){
        return 1;
      } else {
        var nextState = transition(state, action),
            nextAleph = propagateAspiration(state, aleph, action, Edel, nextState);
        return 1 + V_ones(nextState, nextAleph);  // ! recursion here !
      }
    }}));
  });
  // ! recursive function:
  var V_ones = dp.cache(function(state, aleph){
    if (debug) console.log("  V_ones", state, aleph);
    return expectation(Infer({ model() { 
      return Q_ones(state,  // ! recursion here !
                   sample(localPolicy(state, aleph)),  // ! recursion here ! 
                   aleph);
    }}));
  });

  var propagateAspiration = // dp.cache(  // caching this easy to compute function would only clutter the cache due to its many arguments
   function(state, aleph, action, Edel, nextState){
    if (debug) console.log(" propagateAspiration", state, aleph, action, Edel, nextState);

    // the propagated aspiration is a mixture of a steadfast and a rescaled part:

    var steadfastAleph4nextState = _.isArray(aleph) 
          ? [aleph[0] - Edel, aleph[1] - Edel] 
          : aleph - Edel;

    if (rescalingDegree == 0.0) {
      return steadfastAleph4nextState;
    } else {
      // recover the aspiration that we had after choosing action, before knowing which state we would land in:
      // (Note: We have E(q | action ~ localPolicy) = v) by construction of localPolicy in getLocaPolicy().)
      var aleph4state = getAspiration4state(state, aleph),
          aleph4action = getAspiration4action(state, action, aleph4state);
      // compute the relative position of that target in the expectation that we had of 
      //    r plus next feasibility interval 
      // before we knew which state we would land in:
      var lam = relativePosition(minFeasibleQ(state, action), aleph4action, maxFeasibleQ(state, action)); 
      // (this is between 0 and 1 by definition of q.)
      // rescale the target to the feasibility interval of the state that we landed in:
      var rescaledAleph4nextState = interpolate(minFeasibleV(nextState), lam, maxFeasibleV(nextState));
      // (only this part preserves aspiration in expectation)
      return interpolate(steadfastAleph4nextState, rescalingDegree, rescaledAleph4nextState);      
    }
   }
  // )
  ;


  // Other safety criteria:

  // Squared deviation of local relative aspiration from 0.5:
  // recursive function:
  var LRAdev_state = dp.cache(function(state, aleph) {
    var localPolicy = localPolicy(state, aleph);
    return expectation(Infer({ model() { 
      return LRAdev_action(state, sample(localPolicy), aleph); // ! recursion here !
    }}));
  });
  // recursive function:
  var LRAdev_action = dp.cache(function(state, action, aleph) {
    var res = expectation(Infer({ model() {
          var aleph4action = getAspiration4action(state, action, getAspiration4state(state, aleph)),
              aleph4actionMid = _.isArray(aleph4action) ? (aleph4action[0] + aleph4action[1]) / 2 : aleph4action,
              localLRAdev = squared(0.5 - relativePosition(minFeasibleQ(state, action), aleph4actionMid, maxFeasibleQ(state, action)));
          if (state.terminateAfterAction){
            return localLRAdev;
          } else {
            var nextState = transition(state, action),
                nextAleph = propagateAspiration(state, aleph, action, expectedDelta(state, action), nextState);
            return localLRAdev + LRAdev_state(nextState, nextAleph); // ! recursion here !
          }
        }}));
    return res;
  });

  // Shannon entropy of behavior
  // (actually, negative KL divergence relative to uninformedPolicy (e.g., a uniform distribution),
  // to be consistent under action cloning or action refinement):
  // recursive function:
  var behaviorEntropy_state = dp.cache(function(state, aleph) {
    var localPolicy = localPolicy(state, aleph); // ! recursion here !
    return expectation(Infer({ model() { 
      var action = sample(localPolicy);
      return behaviorEntropy_action(state, Math.exp(localPolicy.score(action)), action, aleph); // ! recursion here !
    }}));
  });
  // recursive function:
  var behaviorEntropy_action = dp.cache(function(state, actionProbability, action, aleph) {
    var res = expectation(Infer({ model() {
          var localEntropy = (uninformedPolicy ? uninformedPolicy(state).score(action) : 0) 
                  - Math.log(actionProbability)
                  + (internalActionEntropy ? internalActionEntropy(state, action) : 0);
          if (state.terminateAfterAction){
            return localEntropy;
          } else {
            var nextState = transition(state, action),
                nextAleph = propagateAspiration(state, aleph, action, expectedDelta(state, action), nextState);
            return localEntropy + behaviorEntropy_state(nextState, nextAleph); // ! recursion here !
          }
        }}));
    return res;
  });

  // KL divergence of behavior relative to refPolicy:
  // recursive function:
  var behaviorKLdiv_state = dp.cache(function(state, aleph) {
    var localPolicy = localPolicy(state, aleph); // ! recursion here !
    return expectation(Infer({ model() { 
      var action = sample(localPolicy);
      return behaviorKLdiv_action(state, Math.exp(localPolicy.score(action)), action, aleph); // ! recursion here !
    }}));
  });
  // recursive function:
  var behaviorKLdiv_action = dp.cache(function(state, actionProbability, action, aleph) {
    var ref = refPolicy(state),
        res = expectation(Infer({ model() {
          var localDivergence = Math.log(actionProbability) - ref.score(action);
          if (state.terminateAfterAction){
            return localDivergence;
          } else {
            var nextState = transition(state, action),
                nextAleph = propagateAspiration(state, aleph, action, expectedDelta(state, action), nextState);
            return localDivergence + behaviorKLdiv_state(nextState, nextAleph); // ! recursion here !
          }
        }}));
    return res;
  });

  // state's messing potential (maximal entropy (relative to some uninformedStatePrior) 
  // over trajectories any agent could produce from here):
  // recursive function:
  var messingPotential_state = dp.cache(function(state) {
    // FIXME: this can probably be simplified tremendously. I think the expectation is equal to the log of the normalization constant of maxMPpolicy, or its negative:
    var actions = stateToActions(state),
        weights = map(function(a) { return Math.exp(messingPotential_action(state, a)); }, actions),
        maxMPpolicy = Categorical({vs: actions, ps: weights});
    var MP = expectation(Infer({ model() {
              var action = sample(maxMPpolicy); 
              return messingPotential_action(state, action) - maxMPpolicy.score(action); // ! recursion here !
          }}));
    return MP;
  });
  // recursive function:
  var messingPotential_action = dp.cache(function(state, action) {
    var MP = expectation(Infer({ model() {
          if (state.terminateAfterAction){
            return 0;
          } else {
            var nextState = transition(state, action),
                nextStateScore = transitionDistribution(state, action).score(nextState),
                nextMP = messingPotential_state(nextState); // ! recursion here !
            return nextMP + (uninformedStatePriorScore ? uninformedStatePriorScore(state) : 0) - nextStateScore
                   + (internalTransitionEntropy ? internalTransitionEntropy(state, action, nextState) : 0);
          }
        }}));
    return MP;
  });
  // TODO: later the following should actually be provided by the env/world:
  var transitionDistribution = dp.cache(function(state, action) {
    return Infer({ model() {return transition(state, action); }});
  });

  // other loss:
  // recursive function:
  var otherLoss_state = dp.cache(function(state, aleph) {
    var localPolicy = localPolicy(state, aleph); // ! recursion here !
    return expectation(Infer({ model() { 
      return otherLoss_action(state, sample(localPolicy), aleph)  // ! recursion here !
    }}));
  });
  // recursive function:
  var otherLoss_action = dp.cache(function(state, action, aleph) {
    var ref = refPolicy(state),
        loss = expectation(Infer({ model() {
          var localLoss = otherLocalLoss(state, action);
          if (state.terminateAfterAction){
            return localLoss;
          } else {
            var nextState = transition(state, action),
                nextAleph = propagateAspiration(state, aleph, action, expectedDelta(state, action), nextState);
            return localLoss + otherLoss_state(nextState, nextAleph); // ! recursion here !
          }
        }}));
    return loss;
  });

  var randomTieBreaker = dp.cache(function(state, action) { return Math.random(); });

  // combined loss:
  // ! recursive function:
  var combinedLoss = dp.cache(function(state, action, aleph, v, q, actionProbability) { 
    var q_ones = lossCoeff4DeltaVariation > 0 ? Q_ones(state, action, aleph) : undefined;
    return (
      (lossCoeff4variance > 0 ? lossCoeff4variance * (
          Q2(state, action, aleph) - squared(q) + squared(q - v)
        ) : 0) // ! recursion here !
      + (lossCoeff4DeltaVariation > 0 ? lossCoeff4DeltaVariation * (
          Q_DeltaSquare(state, action, aleph) / q_ones -  Q2(state, action, aleph) / squared(q_ones)
        ) : 0) // ! recursion here !
      + (lossCoeff4LRA > 0 ? lossCoeff4LRA * LRAdev_action(state, action, aleph) : 0)
      + (lossCoeff4MP > 0 ? lossCoeff4MP * messingPotential_action(state, action) : 0)
      + (lossCoeff4KLdiv > 0 ? lossCoeff4KLdiv * behaviorKLdiv_action(state, actionProbability, action, aleph) : 0) 
      + (lossCoeff4entropy > 0 ? lossCoeff4entropy * behaviorEntropy_action(state, actionProbability, action, aleph) : 0)
      + (lossCoeff4feasibilityPower > 0 ? lossCoeff4feasibilityPower * squared(maxFeasibleQ(state, action) - minFeasibleQ(state, action)) : 0)
      + (otherLocalLoss && (lossCoeff4otherLoss > 0) ? lossCoeff4otherLoss * otherLoss_action(state, action, aleph) : 0)
      + (lossCoeff4random > 0 ? lossCoeff4random * randomTieBreaker(state, action) : 0)
    );
  });

  return { 
    // TODO: rename fields to match internal names (breaking change!)
    minFeasibleQ, maxFeasibleQ, minFeasibleV, maxFeasibleV,
    getLocalPolicy: localPolicy, localPolicyData,
    propagateAspiration, getAspiration4action, getAspiration4state, 
    Q, V, Q2, V2, Q_DeltaSquare, V_DeltaSquare, Q_ones, V_ones,
    messingPotential: messingPotential_state, messingPotential_action,
    entropy: behaviorEntropy_state, entropyAction: behaviorEntropy_action, 
    KLdiv: behaviorKLdiv_state, KLdivAction: behaviorKLdiv_action, otherLoss: otherLoss_state, otherLossAction: otherLoss_action, 
    getLoss: combinedLoss
  };
};


// Auxiliary functions:

var asInterval = function(x) { return _.isArray(x) ? x : [x,x]; };

var interpolate = function(x, lam, y) {
  // this is denoted  x : lam : y  in formulas
  if (_.isArray(x) || _.isArray(lam) || _.isArray(y)) {
    // one argument is an interval, so everything becomes an interval:
    var xx = asInterval(x), lamlam = asInterval(lam), yy = asInterval(y);
    return [xx[0] + lamlam[0] * (yy[0] - xx[0]), 
            xx[1] + lamlam[1] * (yy[1] - xx[1])];
  } else {
    return x + lam * (y - x);
  }
}

var relativePosition = function(x, z, y) {
  // this is denoted  x \ z \ y  in formulas
if (_.isArray(x) || _.isArray(z) || _.isArray(y)) {
    // one argument is an interval, so everything becomes an interval:
    var xx = asInterval(x), zz = asInterval(z), yy = asInterval(y);
    return [yy[0] != xx[0] ? (zz[0] - xx[0]) / (yy[0] - xx[0]) : 0.5, 
            yy[1] != xx[1] ? (zz[1] - xx[1]) / (yy[1] - xx[1]) : 0.5];
  } else {
    return y != x ? (z - x) / (y - x) : 0.5;
  }
}

var clip = function(x, z, y) {
  // this is denoted  x[ z ]y  in formulas
if (_.isArray(x) || _.isArray(z) || _.isArray(y)) {
    // one argument is an interval, so everything becomes an interval:
    var xx = asInterval(x), zz = asInterval(z), yy = asInterval(y);
    return [Math.min(Math.max(xx[0], zz[0]), yy[0]),
            Math.min(Math.max(xx[1], zz[1]), yy[1])];
  } else {
    return Math.min(Math.max(x, z), y);
  }
}

var intersect = function(interval1, interval2) {
  // do the two intervals intersect in at least one point?
  return (interval1[0] <= interval2[1]) && (interval2[0] <= interval1[1]);
}

var squared = function(x) { return Math.pow(x, 2); };