// SatisfIA aspiration-based non-maximizing agent 
// with aspiration rescaling and several adjustable safety criteria.

var makeMDPAgentSatisfia = function(params_, world) {

    // uncomment later once we want to do anything with the metalog distribution:
    // var metalog = getMetalog(); // needed to enable metalog distribution used in loss function
    
    var stateActionPairsSet = webpplAgents.emptySet(); // to be able to later loop over all state-action pairs visited

    // extend default parameters and options by supplied ones:
    var params = extend(extend({
      maxLambda: 1, // upper bound on local relative aspiration in each step (must be minLambda...1)
      minLambda: 0, // lower bound on local relative aspiration in each step (must be 0...maxLambda)
      lossCoeff4Cup: 1, // weight of "cup" loss component, must be >= 0
      lossCoeff4Variance: 0, // weight of variance of total in loss function, must be >= 0
      lossCoeff4Fourth: 0, // weight of centralized fourth moment of total in loss function, must be >= 0
      lossCoeff4DeltaVariation: 0, // weight of variation of Delta in loss function, must be >= 0
      lossCoeff4LRA: 0, // weight of deviation of LRA from 0.5 in loss function, must be >= 0
      lossCoeff4MP: 0, // weight of messing potential in loss function, must be >= 0
      lossCoeff4Entropy: 0, // weight of entropy in loss function, must be >= 0
      lossCoeff4KLdiv: 0, // weight of KL divergence in loss function, must be >= 0
      lossCoeff4FeasibilityPower: 0, // weight of power of squared feasibility interval width in loss function, must be >= 0
      lossCoeff4OtherLoss: 0, // weight of other loss components specified by otherLossIncrement, must be >= 0
      lossCoeff4Random: 1e-10, // weight of random tie breaker in loss function, must be >= 0
      allowNegativeCoeffs: false, // if true, allow negative loss coefficients
      onlyUseClosestActions: true, // if true, only use the two actions that are closest to the target aspiration in expectation
      softmaxInvTemp: 1, // inverse temperature of softmax mixture of actions, must be > 0
      rescalingDegree: 1, // degree (0...1) of aspiration rescaling. (expectation is only preserved if this is 1.0)
    }, params_), { options: extend({
      debug: false
    }, params_.options) });
  
    // extract parameters, options, and world:
    var stateToActions = world.stateToActions,  // function(state)
        transition = world.transition;  // stochastic function(state, action)
    var expectedDelta = params.expectedDelta,  // function(state, action)
        varianceOfDelta = params.varianceOfDelta 
          || function(state, action) { return 0; },  // function(state, action)
        skewnessOfDelta = params.skewnessOfDelta
          || function(state, action) { return 0; },  // function(state, action)
        excessKurtosisOfDelta = params.excessKurtosisOfDelta
          || function(state, action) { return 0; },  // function(state, action)
        fifthMomentOfDelta = params.fifthMomentOfDelta
          || function(state, action) { 
            return 8*Math.pow(varianceOfDelta(state, action),2.5); // this assumes a Gaussian distribution
          },  // function(state, action)
        sixthMomentOfDelta = params.sixthMomentOfDelta
          || function(state, action) { 
            return 15*Math.pow(varianceOfDelta(state, action),3); // this assumes a Gaussian distribution
          },  // function(state, action)
        minLambda = params.minLambda, 
        maxLambda = params.maxLambda,
        uninformedStatePriorScore = params.uninformedStatePriorScore,
        uninformedPolicy = params.uninformedPolicy,  // function(state)
        refPolicy = params.refPolicy,  // function(state)
        internalActionEntropy = params.internalActionEntropy,  // function(state, action)
        internalTransitionEntropy = params.internalTransitionEntropy,  // function(state, action, nextState)
        otherLocalLoss = params.otherLossIncrement,  // function(state, action, aleph, v, q, p)
        lossCoeff4Cup = params.lossCoeff4Cup,
        lossCoeff4Variance = params.lossCoeff4Variance,
        lossCoeff4Fourth = params.lossCoeff4Fourth,
        lossCoeff4DeltaVariation = params.lossCoeff4DeltaVariation,
        lossCoeff4LRA = params.lossCoeff4LRA,
        lossCoeff4MP = params.lossCoeff4MP,
        lossCoeff4Entropy = params.lossCoeff4Entropy,
        lossCoeff4KLdiv = params.lossCoeff4KLdiv,
        lossCoeff4FeasibilityPower = params.lossCoeff4FeasibilityPower,
        lossCoeff4OtherLoss = params.lossCoeff4OtherLoss,
        lossCoeff4Random = params.lossCoeff4Random,
        allowNegativeCoeffs = params.allowNegativeCoeffs,
        onlyUseClosestActions = params.onlyUseClosestActions,
        softmaxInvTemp = params.softmaxInvTemp,
        rescalingDegree = params.rescalingDegree;
    var options = params.options, 
        debug = options.debug;
  
    assert.ok(allowNegativeCoeffs | lossCoeff4Cup >= 0, "lossCoeff4Cup must be >= 0");
    assert.ok(allowNegativeCoeffs | lossCoeff4Variance >= 0, "lossCoeff4variance must be >= 0");
    assert.ok(allowNegativeCoeffs | lossCoeff4Fourth >= 0, "lossCoeff4Fourth must be >= 0");
    assert.ok(allowNegativeCoeffs | lossCoeff4DeltaVariation >= 0, "lossCoeff4DeltaVariation must be >= 0");
    assert.ok(allowNegativeCoeffs | lossCoeff4LRA >= 0, "lossCoeff4LRA must be >= 0");
    assert.ok(allowNegativeCoeffs | lossCoeff4MP >= 0, "lossCoeff4MP must be >= 0");
    assert.ok(allowNegativeCoeffs | lossCoeff4Entropy >= 0, "lossCoeff4entropy must be >= 0");
    assert.ok(allowNegativeCoeffs | lossCoeff4KLdiv >= 0, "lossCoeff4KLdiv must be >= 0");
    assert.ok(allowNegativeCoeffs | lossCoeff4FeasibilityPower >= 0, "lossCoeff4FeasibilityPower must be >= 0");
    assert.ok(allowNegativeCoeffs | lossCoeff4OtherLoss >= 0, "lossCoeff4OtherLoss must be >= 0");
    assert.ok(allowNegativeCoeffs | lossCoeff4Random >= 0, "lossCoeff4random must be >= 0");
    assert.ok(softmaxInvTemp > 0, "softmaxInvTemp must be > 0");
    assert.ok(0 <= rescalingDegree <= 1, "rescalingDegree must be in 0...1");
  
    if (debug) console.log("makeMDPAgentSatisfia using parameters", params);
  
    /*  The dependency/callback graph of the following functions is partially recursive 
        and involves aggregation (MIN, MAX, E) operations as follows:
  
        simulate
        → localPolicy
          → aspiration4state
            → minFeasibleV, maxFeasibleV
              → MIN(minFeasibleQ), MAX(maxFeasibleQ)
                → E(minFeasibleV), E(maxFeasibleV) (RECURSION)
          → aspiration4action
            → minFeasibleQ, maxFeasibleQ
          → combinedLoss
            → Q, Q2, Q_DeltaSquare, Q_ones
              → propagateAspiration (see below)
              → E(V), E(V2), E(V_DeltaSquare), E(V_ones)
                → localPolicy (RECURSION)
                → E(Q), E(Q2), E(Q_DeltaSquare), E(Q_ones) (RECURSION)
            → otherLoss_action
              → propagateAspiration (see below)
              → E(otherLoss_state)
                → localPolicy (RECURSION)
                → E(otherLoss_action) (RECURSION)
            → similarly for other loss components (RECURSION)
        → expectedDelta, varianceOfDelta, transition
        → propagateAspiration
          → aspiration4state
          → aspiration4action
        → simulate (RECURSION)
    */
  
    // ! recursive function:
    var localPolicy = function(state, aleph) {
      var policyData = localPolicyData(state, aleph);
      if (debug) console.log(" localPolicy", pretty(state), aleph, pretty(policyData));
      return Categorical({vs: policyData[0], ps: policyData[1]});
    }
    var localPolicyData = dp.cache(function(state, aleph){
      // The following quantities are potentially intervals: aleph, v, Qstate(a)
  
      // Use aspiration aleph to set the desired expected total for this state before (!) having chosen an action:
      var aleph4state = aspiration4state(state, aleph),
          actions = stateToActions(state),
          Qstate = function(action) { return aspiration4action(state, action, aleph4state); };
      // Meet point v or midpoint of interval v with a mixture of at most two actions:
      var vMid = _.isArray(aleph4state) ? (aleph4state[0] + aleph4state[1]) / 2 : aleph4state,
          QstateMid = _.isArray(aleph4state) ? function(a) { var q = Qstate(a); return (q[0] + q[1]) / 2; } : Qstate;
  
      // TODO: make closest/all and softmin/argmin independent options:
  
      if (onlyUseClosestActions) {
        
        // Find the two QstateMid table entries, qHi and qLo, that are closest to vMid from above and below: 
        var qHi = minWith(function(action) {return QstateMid(action) >= vMid ? QstateMid(action) : 1e10}, actions)[1],
            qLo = maxWith(function(action) {return QstateMid(action) <= vMid ? QstateMid(action) : -1e10}, actions)[1];
        var actionsHi = filter(function(action) {return QstateMid(action) == qHi}, actions),
            actionsLo = filter(function(action) {return QstateMid(action) == qLo}, actions);
        // Our local policy that guarantees v in expectation is a suitable mixture of two actions with those Q values:
        var probHi = relativePosition(qLo, vMid, qHi);
        if (debug) console.log("  qHi, qLo, actionsHi, actionsLo, p", qHi, qLo, actionsHi, actionsLo, probHi);
  
        // From those actions that meet these values, choose a pair that optimizes 
        // an additional safety criterion encoded in a loss function (see below):
        var lossHi = function(action) { return combinedLoss(state, action, aleph, vMid, qHi, probHi); },
            lossLo = function(action) { return combinedLoss(state, action, aleph, vMid, qLo, 1-probHi); },
            actionHi = minWith(lossHi, actionsHi)[0],
            actionLo = minWith(lossLo, actionsLo)[0];
        return actionHi == actionLo ? [[actionHi], [1]] : [[actionHi, actionLo], [probHi, 1 - probHi]];
  
      } else {
        
        // Use a softmax mixture of all actions, based on the safety loss.
        // (Note: this will lead to larger entropy and will not (?) behave as consistently under MDP refinements
        // in which an action is resolved into a sequence of sub-actions.)
  
        // First devide actions into those that are above and below vMid:
        var actionsHi = filter(function(action) {return QstateMid(action) >= vMid}, actions),
            actionsLo = filter(function(action) {return QstateMid(action) <= vMid}, actions);
        // Calculate an approximate relative mixture coefficient p for upper group, to be used for approximating KLdiv:
        var approxQHi = expectation(Infer({ model() { return QstateMid(uniformDraw(actionsHi)); }})),
            approxQLo = expectation(Infer({ model() { return QstateMid(uniformDraw(actionsLo)); }})),
            approxP = clip(1e-10, approxQHi > approxQLo ? (vMid - approxQLo) / (approxQHi - approxQLo) : 0.5, 1-1e-10);
        // Calculate loss for each action:          
        var lossHi = function(a) { return combinedLoss(state, a, aleph, vMid, approxQHi, approxP); },
            lossLo = function(a) { return combinedLoss(state, a, aleph, vMid, approxQLo, 1-approxP); };
        if (debug) console.log("  approxQHi, approxQLo, approxP, actionsHi, lossHi, actionsLo, lossLo", approxQHi, approxQLo, approxP, actionsHi, map(lossHi, actionsHi), actionsLo, map(lossLo, actionsLo));
        // Construct softmax distributions localPolicyHi, localPolicyLo over actionsHi, actionsLo:
        var defaultPolicy = uninformedPolicy ? uninformedPolicy(state) : undefined,
            probabilityFct = function(a) {
              var res = Math.exp((defaultPolicy ? defaultPolicy.score(a) : 0) - softmaxInvTemp * lossHi(a));
              return res < Infinity ? (res > 0 ? res : 1e-10) : 1e10;
            },
            psHi = map(probabilityFct, actionsHi),
            psLo = map(probabilityFct, actionsLo);
        if (debug) console.log("  psHi, psLo", psHi, psLo);
        var localPolicyHi = Categorical({vs: actionsHi, ps: psHi}),
            localPolicyLo = Categorical({vs: actionsLo, ps: psLo});  
        // Calculate the true qHi, qLo, and p from these distributions:
        var qHi = expectation(localPolicyHi, QstateMid),
            qLo = expectation(localPolicyLo, QstateMid),
            probHi = clip(1e-10, relativePosition(qLo, vMid, qHi), 1-1e-10);
        // Finally, mix the two local policies with the true p:
        var localPolicy = Mixture({ps: [probHi, 1 - probHi], dists: [localPolicyHi, localPolicyLo]});
        // To use the cache efficiently, we return the policy as a pair of arrays:
        var actionsTaken = localPolicy.support(),
            probabilityFct = function(a) { return Math.exp(localPolicy.score(a)) },
            probabilities = map(probabilityFct, actionsTaken);
        return [actionsTaken, probabilities];
  
      }
    });
  
    // Compute upper and lower feasibility bounds for Q and V that are allowed in view of maxLambda and minLambda:
  
    // Compute the Q and V functions of the classical maximization problem (if maxLambda==1)
    // or of the LRA-based problem (if maxLambda<1):
    // ! recursive function:
    var maxFeasibleQ = dp.cache(function(state, action){
      // TODO: register (state, action) in global store
      stateActionPairsSet.add([state,action]);
      var Edel = expectedDelta(state, action),
          q = state.terminateAfterAction 
              ? Edel 
              : Edel + expectation(Infer({ model() { 
                return maxFeasibleV(transition(state, action));  // ! recursion here ! 
              }}));
      if (debug) console.log("   maxFeasibleQ", pretty(state), action, q);
      return q;
    });
    // ! recursive function:
    var maxFeasibleV = dp.cache(function(state){
      var actions = stateToActions(state),
          Qstate = function(a) { return maxFeasibleQ(state, a); },  // ! recursion here !
          v = maxLambda == 1 
              ? maxWith(Qstate, actions)[1] 
              : interpolate(minWith(Qstate, actions)[1], maxLambda, maxWith(Qstate, actions)[1]);
      if (debug) console.log("   maxFeasibleV", pretty(state), v);
      return v;
    });
  
    // Compute the Q and V functions of the corresponding minimization (!) problem (if minLambda==0)
    // or of the LRA-based problem (if minLambda>0):
    // ! recursive function:
    var minFeasibleQ = dp.cache(function(state, action){
      var Edel = expectedDelta(state, action),
          q = state.terminateAfterAction 
              ? Edel 
              : Edel + expectation(Infer({ model() { 
                return minFeasibleV(transition(state, action));  // ! recursion here !
              }}));
      if (debug) console.log("   minFeasibleQ", pretty(state), action, q);
      return q;
    });
    // ! recursive function:
    var minFeasibleV = dp.cache(function(state){
      var actions = stateToActions(state),
          Qstate = function(action) { return minFeasibleQ(state, action); },  // ! recursion here !
          v = minLambda == 0 
              ? minWith(Qstate, actions)[1] 
              : interpolate(minWith(Qstate, actions)[1], minLambda, maxWith(Qstate, actions)[1]);
      if (debug) console.log("   minFeasibleV", pretty(state), v);
      return v;
    });
  
    // When using action in state, we can get any expected total in the interval
    // [minFeasibleQ(state, action), maxFeasibleQ(state, action)].
    var aspiration4action = dp.cache(function(state, action, aleph4state){
      var qLo = minFeasibleQ(state, action),
          qHi = maxFeasibleQ(state, action);
      if (!_.isArray(aleph4state)) {
        // So when having aspiration aleph, we can still fulfill it in expectation if it lies in the interval.
        // Therefore, when using action in state at aspiration aleph, 
        // we set our aspiration when choosing this action to aleph clipped to that interval:
        var res = clip(qLo, aleph4state, qHi);
        if (debug) console.log("  aspiration4action", pretty(state), action, aleph4state, res);
        return res;
      } else {
        // For interval aleph, we choose an interval that guarantees that the resulting mixed interval of all
        // localPolicies that localPolicy() may mix with this action will be within aleph: 
        var alephLo = aleph4state[0], alephHi = aleph4state[1], width = alephHi - alephLo,
            res = [Math.max(qLo, Math.min(alephLo, qHi - width)), 
                   Math.min(qHi, Math.max(alephHi, qLo + width))];
        if (debug) console.log("  aspiration4action", pretty(state), action, aleph4state, res);
        return res;
      }
    });
    // When in state, we can get any expected total in the interval
    // [minFeasibleV(state), maxFeasibleV(state)].
    // So when having aspiration aleph, we can still fulfill it in expectation if it lies in the interval.
    // Therefore, when in state at incoming aspiration aleph, 
    // we adjust our aspiration to aleph clipped to that interval:
    var aspiration4state = dp.cache(function(state, propagatedAleph){
      var res = clip(minFeasibleV(state), propagatedAleph, maxFeasibleV(state));
      if (debug) console.log("  aspiration4state", pretty(state), propagatedAleph, res);
      return res;
    });
  
    // Actual Q and V functions of resulting policy (always returning scalars):
    // ! recursive function:
    var Q = dp.cache(function(state, action, aleph){
      var q = expectation(Infer({ model() {
        var Edel = expectedDelta(state, action);
        if (state.terminateAfterAction){
          return Edel;
        } else {
          var nextState = transition(state, action),
              nextAleph = propagateAspiration(state, aleph, action, Edel, nextState);
          return Edel + V(nextState, nextAleph);  // ! recursion here !
        }
      }})); 
      if (debug) console.log("  Q", pretty(state), action, aleph, q);
      return q;
    });
    // TODO: fix the Infinity error for GW6, webppl --require webppl-dp --require webppl-json --require . restaurant_choice.wppl -- --aleph0 2  --maxLambda 1.0 --minLambda 0.3 --debug 1
    // ! recursive function:
    var V = dp.cache(function(state, aleph){
      var v = expectation(Infer({ model() { 
        return Q(state,  // ! recursion here !
                 sample(localPolicy(state, aleph)),  // ! recursion here ! 
                 aleph);
      }}));
      if (debug) console.log("  V", pretty(state), aleph, v);
      return v;
    });
  
    // TODO: why does the following not work?
    /*
    // helper function used in Bellman-equation-based quantities: 
    var VfromQ = function(someQ) { 
      return dp.cache(function(state, aleph) { 
        return expectation(Infer({ model() { 
          return someQ(state, sample(localPolicy(state, aleph)), aleph); 
        }}));
      }); 
    };
    var V = VfromQ(Q);
    */
  
    var expectedDeltaSquared = dp.cache(function(state, action){
      var Edel = expectedDelta(state, action);
      return varianceOfDelta(state, action) + squared(Edel);
    });
    var expectedDeltaCubed = dp.cache(function(state, action){
      var Edel = expectedDelta(state, action),
          varDel = varianceOfDelta(state, action);
      return Math.pow(varDel, 1.5)*skewnessOfDelta(state, action) + 3*Edel*varDel + Math.pow(Edel, 3);
    });
    var expectedDeltaFourth = dp.cache(function(s, a){
      var Edel = expectedDelta(s, a);
      return squared(varianceOfDelta(s, a)) * (3 + excessKurtosisOfDelta(s, a))
             + 4*expectedDeltaCubed(s, a)*Edel - 6*expectedDeltaSquared(s, a)*squared(Edel) + 3*Math.pow(Edel, 4);
    });
    var expectedDeltaFifth= dp.cache(function(s, a){
      var Edel = expectedDelta(s, a);
      return fifthMomentOfDelta(s, a)
             + 5*expectedDeltaFourth(s, a)*Edel - 10*expectedDeltaCubed(s, a)*squared(Edel) 
             + 10*expectedDeltaSquared(s, a)*cubed(Edel) - 4*Math.pow(Edel, 5);
    });
    var expectedDeltaSixth = dp.cache(function(s, a){
      var Edel = expectedDelta(s, a);
      return sixthMomentOfDelta(s, a)
             + 6*expectedDeltaFifth(s, a)*Edel - 15*expectedDeltaFourth(s, a)*squared(Edel) 
             + 20*expectedDeltaCubed(s, a)*cubed(Edel) 
             - 15*expectedDeltaSquared(s, a)*Math.pow(Edel, 4) + 5*Math.pow(Edel, 6);
    });
  
    // Expected squared total, for computing the variance of total:
    // ! recursive function:
    var Q2 = dp.cache(function(state, action, aleph){
      var q2 = expectation(Infer({ model() {
        var Edel = expectedDelta(state, action),
            Edel2 = expectedDeltaSquared(state, action);
        if (state.terminateAfterAction){
          return Edel2;
        } else {
          var nextState = transition(state, action),
              nextAleph = propagateAspiration(state, aleph, action, Edel, nextState);
          // TODO: verify formula:
          return Edel2 + 2*Edel*V(nextState, nextAleph) + V2(nextState, nextAleph); // ! recursion here !
        }
      }}));
      if (debug) console.log("  Q2", pretty(state), action, aleph, q2);
      return q2;
    });
    // ! recursive function:
    var V2 = dp.cache(function(state, aleph){
      var v2 = expectation(Infer({ model() { 
        return Q2(state,  // ! recursion here !
                  sample(localPolicy(state, aleph)),  // ! recursion here !
                  aleph); 
      }}));
      if (debug) console.log("  V2", pretty(state), aleph, v2);
      return v2;
    });
  
    // Similarly: Expected third and fourth powers of total, for computing the 3rd and 4th centralized moment of total:
    // ! recursive function:
    var Q3 = dp.cache(function(state, action, aleph){
      var q3 = expectation(Infer({ model() {
        var Edel = expectedDelta(state, action),
            Edel2 = expectedDeltaSquared(state, action),
            Edel3 = expectedDeltaCubed(state, action);
        if (state.terminateAfterAction){
          return Edel3;
        } else {
          var nextState = transition(state, action),
              nextAleph = propagateAspiration(state, aleph, action, Edel, nextState);
          // TODO: verify formula:
          return Edel3 + 3*Edel2*V(nextState, nextAleph) 
                 + 3*Edel*V2(nextState, nextAleph) + V3(nextState, nextAleph); // ! recursion here !
        }
      }}));
      if (debug) console.log("  Q3", pretty(state), action, aleph, q3);
      return q3;
    });
    // ! recursive function:
    var V3 = dp.cache(function(state, aleph){
      var v3 = expectation(Infer({ model() {
        return Q3(state,  // ! recursion here !
                  sample(localPolicy(state, aleph)),  // ! recursion here !
                  aleph);
      }}));
      if (debug) console.log("  V3", pretty(state), aleph, v3);
      return v3;
    });
  
    // Expected fourth power of total, for computing the expected fourth power of deviation of total from expected total (= fourth centralized moment of total):
    // ! recursive function:
    var Q4 = dp.cache(function(state, action, aleph){
      var q4 = expectation(Infer({ model() {
        var Edel = expectedDelta(state, action),
            Edel2 = expectedDeltaSquared(state, action),
            Edel3 = expectedDeltaCubed(state, action),
            Edel4 = expectedDeltaFourth(state, action);
        if (state.terminateAfterAction){
          return Edel4;
        } else {
          var nextState = transition(state, action),
              nextAleph = propagateAspiration(state, aleph, action, Edel, nextState);
          // TODO: verify formula:
          return Edel4 + 4*Edel3*V(nextState, nextAleph) 
                 + 6*Edel2*V2(nextState, nextAleph) 
                 + 4*Edel*V3(nextState, nextAleph) + V4(nextState, nextAleph); // ! recursion here !
        }
      }}));
      if (debug) console.log("  Q4", pretty(state), action, aleph, q4);
      return q4;
    });
    // ! recursive function:
    var V4 = dp.cache(function(state, aleph){
      var v4 = expectation(Infer({ model() {
        return Q4(state,  // ! recursion here !
                  sample(localPolicy(state, aleph)),  // ! recursion here !
                  aleph);
      }}));
      if (debug) console.log("  V4", pretty(state), aleph, v4);
      return v4;
    });
  
    // Expected fifth power of total, for computing the bed-and-banks loss component based on a 6th order polynomial potential of this shape: https://www.wolframalpha.com/input?i=plot+%28x%2B1%29%C2%B3%28x-1%29%C2%B3+ :
    // ! recursive function:
    var Q5 = dp.cache(function(state, action, aleph){
      var q5 = expectation(Infer({ model() {
        var Edel = expectedDelta(state, action),
            Edel2 = expectedDeltaSquared(state, action),
            Edel3 = expectedDeltaCubed(state, action),
            Edel4 = expectedDeltaFourth(state, action),
            Edel5 = expectedDeltaFifth(state, action);
        if (state.terminateAfterAction){
          return Edel5;
        } else {
          var nextState = transition(state, action),
              nextAleph = propagateAspiration(state, aleph, action, Edel, nextState);
          // TODO: verify formula:
          return Edel5 + 5*Edel4*V(nextState, nextAleph) + 10*Edel3*V2(nextState, nextAleph) 
                 + 10*Edel2*V3(nextState, nextAleph) + 5*Edel*V4(nextState, nextAleph) + V5(nextState, nextAleph); // ! recursion here !
        }
      }}));
      if (debug) console.log("  Q5", pretty(state), action, aleph, q5);
      return q5;
    });
    // ! recursive function:
    var V5 = dp.cache(function(state, aleph){
      var v5 = expectation(Infer({ model() {
        return Q5(state,  // ! recursion here !
                  sample(localPolicy(state, aleph)),  // ! recursion here !
                  aleph);
      }}));
      if (debug) console.log("  V5", pretty(state), aleph, v5);
      return v5;
    });
    
    // Expected sixth power of total, for computing the bed-and-banks loss component based on a 6th order polynomial potential of this shape: https://www.wolframalpha.com/input?i=plot+%28x%2B1%29%C2%B3%28x-1%29%C2%B3+ :
    // ! recursive function:
    var Q6 = dp.cache(function(state, action, aleph){
      var q6 = expectation(Infer({ model() {
        var Edel = expectedDelta(state, action),
            Edel2 = expectedDeltaSquared(state, action),
            Edel3 = expectedDeltaCubed(state, action),
            Edel4 = expectedDeltaFourth(state, action),
            Edel5 = expectedDeltaFifth(state, action),
            Edel6 = expectedDeltaSixth(state, action);
        if (state.terminateAfterAction){
          return Edel6;
        } else {
          var nextState = transition(state, action),
              nextAleph = propagateAspiration(state, aleph, action, Edel, nextState);
          // TODO: verify formula:
          return Edel6 + 6*Edel5*V(nextState, nextAleph) + 15*Edel4*V2(nextState, nextAleph) 
                 + 20*Edel3*V3(nextState, nextAleph) 
                 + 15*Edel2*V4(nextState, nextAleph) + 6*Edel*V5(nextState, nextAleph) + V6(nextState, nextAleph); // ! recursion here !
        }
      }}));
      if (debug) console.log("  Q6", pretty(state), action, aleph, q6);
      return q6;
    });
    // ! recursive function:
    var V6 = dp.cache(function(state, aleph){
      var v6 = expectation(Infer({ model() {
        return Q6(state,  // ! recursion here !
                  sample(localPolicy(state, aleph)),  // ! recursion here !
                  aleph);
      }}));
      if (debug) console.log("  V6", pretty(state), aleph, v6);
      return v6;
    });

    // Expected powers of difference between total and some target value v,
    // needed for calculating moments of probabilistic policies in loss function:

    /* currently unused:
        var Qv = dp.cache(function(s, // state
                                a, // action
                                al, // aleph
                                v // target value
                ){
            return Q(s,a,al) - v;
        });
    */
    var Q2v = dp.cache(function(s, a, al, v){
        return Q2(s,a,al) - 2*Q(s,a,al)*v + squared(v);
    });
    /*
        var Q3v = dp.cache(function(s, a, al, v){
            return Q3(s,a,al) - 3*Q2(s,a,al)*v + 3*Q(s,a,al)*squared(v) - cubed(v);
        });
    */
    var Q4v = dp.cache(function(s, a, al, v){
        return Q4(s,a,al) - 4*Q3(s,a,al)*v + 6*Q2(s,a,al)*squared(v) - 4*Q(s,a,al)*cubed(v) + Math.pow(v,4);
    });
    /*    
        var Q5v = dp.cache(function(s, a, al, v){
            return Q5(s,a,al) - 5*Q4(s,a,al)*v + 10*Q3(s,a,al)*squared(v) 
                - 10*Q2(s,a,al)*cubed(v) + 5*Q(s,a,al)*Math.pow(v,4) - Math.pow(v,5);
        }); 
    */
    var Q6v = dp.cache(function(s, a, al, v){
        return Q6(s,a,al) - 6*Q5(s,a,al)*v + 15*Q4(s,a,al)*squared(v)
               - 20*Q3(s,a,al)*cubed(v) 
               + 15*Q2(s,a,al)*Math.pow(v,4) - 6*Q(s,a,al)*Math.pow(v,5) + Math.pow(v,6);
    });
    
    // TODO: verify the formulas for expected Delta variation along a trajectory:
    // Expected total of squared Deltas, for computing the expected Delta variation along a trajectory:
    // ! recursive function:
    var Q_DeltaSquare = dp.cache(function(state, action, aleph){
      var qDsq = expectation(Infer({ model() {
        var Edel = expectedDelta(state, action),
            EdelSq = squared(Edel) + varianceOfDelta(state, action);
        if (state.terminateAfterAction){
          return EdelSq;
        } else {
          var nextState = transition(state, action),
              nextAleph = propagateAspiration(state, aleph, action, Edel, nextState);
          return EdelSq + V_DeltaSquare(nextState, nextAleph);  // ! recursion here !
        }
      }}));
      if (debug) console.log("  Q_DeltaSquare", pretty(state), action, aleph, qDsq);
      return qDsq;
    });
    // ! recursive function:
    var V_DeltaSquare = dp.cache(function(state, aleph){
      var vDsq = expectation(Infer({ model() { 
        return Q_DeltaSquare(state,  // ! recursion here !
                             sample(localPolicy(state, aleph)),  // ! recursion here ! 
                             aleph);
      }}));
      if (debug) console.log("  V_DeltaSquare", pretty(state), aleph, vDsq);
      return vDsq;
    });
  
    // Expected total of ones (= expected length of trajectory), for computing the expected Delta variation along a trajectory:
    // ! recursive function:
    var Q_ones = dp.cache(function(state, action, aleph){
      var q_ones = expectation(Infer({ model() {
        var Edel = expectedDelta(state, action);
        if (state.terminateAfterAction){
          return 1;
        } else {
          var nextState = transition(state, action),
              nextAleph = propagateAspiration(state, aleph, action, Edel, nextState);
          return 1 + V_ones(nextState, nextAleph);  // ! recursion here !
        }
      }}));
      if (debug) console.log("  Q_ones", pretty(state), action, aleph, q_ones);
      return q_ones;
    });
    // ! recursive function:
    var V_ones = dp.cache(function(state, aleph){
      var v_ones = expectation(Infer({ model() { 
        return Q_ones(state,  // ! recursion here !
                     sample(localPolicy(state, aleph)),  // ! recursion here ! 
                     aleph);
      }}));
      if (debug) console.log("  V_ones", pretty(state), aleph, v_ones);
      return v_ones;
    });
  
    var propagateAspiration = // dp.cache(  // caching this easy to compute function would only clutter the cache due to its many arguments
     function(state, aleph, action, Edel, nextState){  
      // the propagated aspiration is a mixture of a steadfast and a rescaled part:
  
      var steadfastAleph4nextState = _.isArray(aleph) 
            ? [aleph[0] - Edel, aleph[1] - Edel] 
            : aleph - Edel;
  
      if (rescalingDegree == 0.0) {
        var res = steadfastAleph4nextState;
        if (debug) console.log(" propagateAspiration", pretty(state), aleph, action, Edel, pretty(nextState), res);
        return res;
      } else {
        // recover the aspiration that we had after choosing action, before knowing which state we would land in:
        // (Note: We have E(q | action ~ localPolicy) = v) by construction of localPolicy in getLocaPolicy().)
        var aleph4state = aspiration4state(state, aleph),
            aleph4action = aspiration4action(state, action, aleph4state);
        // compute the relative position of that target in the expectation that we had of 
        //    r plus next feasibility interval 
        // before we knew which state we would land in:
        var lam = relativePosition(minFeasibleQ(state, action), aleph4action, maxFeasibleQ(state, action)); 
        // (this is between 0 and 1 by definition of q.)
        // rescale the target to the feasibility interval of the state that we landed in:
        var rescaledAleph4nextState = interpolate(minFeasibleV(nextState), lam, maxFeasibleV(nextState));
        // (only this part preserves aspiration in expectation)
        var res = interpolate(steadfastAleph4nextState, rescalingDegree, rescaledAleph4nextState);      
        if (debug) console.log(" propagateAspiration", pretty(state), aleph, action, Edel, pretty(nextState), res);
        return res;
      }
     }
    // )
    ;
  
    // Other safety criteria:
  
    // Squared deviation of local relative aspiration from 0.5:
    // recursive function:
    var LRAdev_state = dp.cache(function(state, aleph) {
      var localPolicy = localPolicy(state, aleph);
      return expectation(Infer({ model() { 
        return LRAdev_action(state, sample(localPolicy), aleph); // ! recursion here !
      }}));
    });
    // recursive function:
    var LRAdev_action = dp.cache(function(state, action, aleph) {
      var res = expectation(Infer({ model() {
            var aleph4action = aspiration4action(state, action, aspiration4state(state, aleph)),
                aleph4actionMid = _.isArray(aleph4action) ? (aleph4action[0] + aleph4action[1]) / 2 : aleph4action,
                localLRAdev = squared(0.5 - relativePosition(minFeasibleQ(state, action), aleph4actionMid, maxFeasibleQ(state, action)));
            if (state.terminateAfterAction){
              return localLRAdev;
            } else {
              var nextState = transition(state, action),
                  nextAleph = propagateAspiration(state, aleph, action, expectedDelta(state, action), nextState);
              return localLRAdev + LRAdev_state(nextState, nextAleph); // ! recursion here !
            }
          }}));
      return res;
    });
  
    // Shannon entropy of behavior
    // (actually, negative KL divergence relative to uninformedPolicy (e.g., a uniform distribution),
    // to be consistent under action cloning or action refinement):
    // recursive function:
    var behaviorEntropy_state = dp.cache(function(state, aleph) {
      var localPolicy = localPolicy(state, aleph); // ! recursion here !
      return expectation(Infer({ model() { 
        var action = sample(localPolicy);
        return behaviorEntropy_action(state, Math.exp(localPolicy.score(action)), action, aleph); // ! recursion here !
      }}));
    });
    // recursive function:
    var behaviorEntropy_action = dp.cache(function(state, actionProbability, action, aleph) {
      var res = expectation(Infer({ model() {
            var defaultPolicy = uninformedPolicy ? uninformedPolicy(state) : undefined,
                localEntropy = (defaultPolicy ? defaultPolicy.score(action) : 0) 
                    - Math.log(actionProbability)
                    + (internalActionEntropy ? internalActionEntropy(state, action) : 0);
            if (state.terminateAfterAction){
              return localEntropy;
            } else {
              var nextState = transition(state, action),
                  nextAleph = propagateAspiration(state, aleph, action, expectedDelta(state, action), nextState);
              return localEntropy + behaviorEntropy_state(nextState, nextAleph); // ! recursion here !
            }
          }}));
      return res;
    });
  
    // KL divergence of behavior relative to refPolicy (or uninformedPolicy if refPolicy is undefined):
    // recursive function:
    var behaviorKLdiv_state = dp.cache(function(state, aleph) {
      var localPolicy = localPolicy(state, aleph); // ! recursion here !
      return expectation(Infer({ model() { 
        var action = sample(localPolicy);
        return behaviorKLdiv_action(state, Math.exp(localPolicy.score(action)), action, aleph); // ! recursion here !
      }}));
    });
    // recursive function:
    var behaviorKLdiv_action = dp.cache(function(state, actionProbability, action, aleph) {
      var ref = refPolicy ? refPolicy(state) : uninformedPolicy ? uninformedPolicy(state) : undefined;
      if (refPolicy) {
        var res = expectation(Infer({ model() {
            var localDivergence = Math.log(actionProbability) - ref.score(action);
            if (state.terminateAfterAction){
              return localDivergence;
            } else {
              var nextState = transition(state, action),
                  nextAleph = propagateAspiration(state, aleph, action, expectedDelta(state, action), nextState);
              return localDivergence + behaviorKLdiv_state(nextState, nextAleph); // ! recursion here !
            }
          }}));
        return res;
      } else {
        return undefined;
      }
    });
  
    // state's messing potential (maximal entropy (relative to some uninformedStatePrior) 
    // over trajectories any agent could produce from here):
    // recursive function:
    var messingPotential_state = dp.cache(function(state) {
      // FIXME: this can probably be simplified tremendously. I think the expectation is equal to the log of the normalization constant of maxMPpolicy, or its negative:
      var actions = stateToActions(state),
          weights = map(function(a) { return Math.exp(messingPotential_action(state, a)); }, actions),
          maxMPpolicy = Categorical({vs: actions, ps: weights});
      var MP = expectation(Infer({ model() {
                var action = sample(maxMPpolicy); 
                return messingPotential_action(state, action) - maxMPpolicy.score(action); // ! recursion here !
            }}));
      return MP;
    });
    // recursive function:
    var messingPotential_action = dp.cache(function(state, action) {
      var MP = expectation(Infer({ model() {
            if (state.terminateAfterAction){
              return 0;
            } else {
              var nextState = transition(state, action),
                  nextStateScore = transitionDistribution(state, action).score(nextState),
                  nextMP = messingPotential_state(nextState); // ! recursion here !
              return nextMP + (uninformedStatePriorScore ? uninformedStatePriorScore(state) : 0) - nextStateScore
                     + (internalTransitionEntropy ? internalTransitionEntropy(state, action, nextState) : 0);
            }
          }}));
      return MP;
    });
    // TODO: later the following should actually be provided by the env/world:
    var transitionDistribution = dp.cache(function(state, action) {
      return Infer({ model() {return transition(state, action); }});
    });
  
    // other loss:
    // recursive function:
    var otherLoss_state = dp.cache(function(state, aleph) {
      var localPolicy = localPolicy(state, aleph); // ! recursion here !
      return expectation(Infer({ model() { 
        return otherLoss_action(state, sample(localPolicy), aleph)  // ! recursion here !
      }}));
    });
    // recursive function:
    var otherLoss_action = dp.cache(function(state, action, aleph) {
      var ref = refPolicy(state),
          loss = expectation(Infer({ model() {
            var localLoss = otherLocalLoss(state, action);
            if (state.terminateAfterAction){
              return localLoss;
            } else {
              var nextState = transition(state, action),
                  nextAleph = propagateAspiration(state, aleph, action, expectedDelta(state, action), nextState);
              return localLoss + otherLoss_state(nextState, nextAleph); // ! recursion here !
            }
          }}));
      return loss;
    });
  
    var randomTieBreaker = dp.cache(function(state, action) { return Math.random(); });
  
    // combined loss:
    // ! recursive function:
    var combinedLoss = dp.cache(function(s /*state*/, a /*action*/, al /*aleph*/, v, q, p /*actionProbability*/) { 
      var q_ones = lossCoeff4DeltaVariation != 0 ? Q_ones(s, a, al) : undefined,
          alI = asInterval(al);
      return (
        (lossCoeff4Cup != 0 ? lossCoeff4Cup * (
            // loss based on a "cup" shaped potential centered at the mid-point of the aspiration interval
            // that is almost completely flat in the middle half of the interval 
            // (https://www.wolframalpha.com/input?i=plot+%28x-.5%29%5E6+from+0+to+1):
            Q6v(s, a, al, (alI[0] + alI[1]) / 2)
          ) : 0) // ! recursion here !
        + (lossCoeff4Variance != 0 ? lossCoeff4Variance * (
            Q2v(s, a, al, v)
          ) : 0) // ! recursion here !
        + (lossCoeff4Fourth != 0 ? lossCoeff4Fourth * (
            Q4v(s, a, al, v)
          ) : 0) // ! recursion here ! 
        + (lossCoeff4DeltaVariation != 0 ? lossCoeff4DeltaVariation * (
            Q_DeltaSquare(s, a, al) / q_ones -  Q2(s, a, al) / squared(q_ones)
          ) : 0) // ! recursion here !
        + (lossCoeff4LRA != 0 ? lossCoeff4LRA * LRAdev_action(s, a, al) : 0)
        + (lossCoeff4MP != 0 ? lossCoeff4MP * messingPotential_action(s, a) : 0)
        + (lossCoeff4KLdiv != 0 ? lossCoeff4KLdiv * behaviorKLdiv_action(s, p, a, al) : 0) 
        + (lossCoeff4Entropy != 0 ? lossCoeff4Entropy * behaviorEntropy_action(s, p, a, al) : 0)
        + (lossCoeff4FeasibilityPower != 0 ? lossCoeff4FeasibilityPower * squared(maxFeasibleQ(s, a) - minFeasibleQ(s, a)) : 0)
        + (otherLocalLoss && (lossCoeff4OtherLoss != 0) ? lossCoeff4otherLoss * otherLoss_action(s, a, al) : 0)
        + (lossCoeff4Random != 0 ? lossCoeff4Random * randomTieBreaker(s, a) : 0)
      );
    });
    // TODO: add further criteria, like:
    // - estd. probability of actual total being in aspiration interval
    // - using same action as last step
    // - using "simple" policies

    var getData = function() {
      var stateActionPairs = Array.from(stateActionPairsSet),
          states = Array.from(webpplAgents.setFrom(map(function(pair) { return pair[0]; }, stateActionPairs)));
      // TODO: locs etc.
      return { states, stateActionPairs };
    };

    return { 
      minFeasibleQ, maxFeasibleQ, minFeasibleV, maxFeasibleV,
      localPolicy, localPolicyData,
      propagateAspiration, aspiration4action, aspiration4state, 
      Q, V, Q2, V2, Q_DeltaSquare, V_DeltaSquare, Q_ones, V_ones,
      messingPotential_state, messingPotential_action,
      behaviorEntropy_state, behaviorEntropy_action, 
      behaviorKLdiv_state, behaviorKLdiv_action, otherLoss_state, otherLoss_action, 
      combinedLoss,
      getData
    };
};
