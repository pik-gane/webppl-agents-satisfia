// SatisfIA aspiration-based non-maximizing agent 
// with aspiration rescaling and several adjustable safety criteria.

// TODO: see https://github.com/orgs/pik-gane/projects/2

/* Note on terminology, variable naming, and notation: 
 * - delta is what others call reward
 * - total is what others call return
 * - phi is a admissibility interval for a state or state-action
 * - aleph is an aspiration interval for a state or state-action
 */

/*

Notes on how to port this WebPPL code to Python:
==================================================

- The main difficulty will be replacing WebPPL's expectation() function 
  by an explicit calculation of the expected value as a weighted sum over
  the support of the distribution. So any occurrence of

      x = expectation(Infer({ model() { 
        ... 
        successor_state = transition(state, action); 
        ... 
        return something
      }}))

  will have to be replaced by something like

      def f(successor_state):
        ...
        return something

      x = sum(map(
        lambda successor_state: transition_prob(state, action, successor_state) * f(successor_state), 
        successor_states
      ))

  And similarly for other sampling functions than transition(), e.g. uniformDraw() or sample(localPolicy).

- The second important thing is to replace WebPPL's dp.cache() by a Python equivalent to avoid multiple
  evaluations of the same function call. This can be done by using a dictionary that maps function arguments
  to function values. So any occurrence of

      var f = dp.cache(function(x, y, z) { ... });

  will have to be replaced by something like

      f_cache = {}
      def f(x, y, z):
        if (x, y, z) not in f_cache:
          f_cache[(x, y, z)] = ...
        return f_cache[(x, y, z)]
        

Notes on how to turn the model-based planning approach into a model-free learning approach:
=============================================================================================

DQN-like approach (try this one first)
----------------------------------------

One approach is similar to DQN, where the main table (Q) was replaced by an ANN.

In our case, we have more than one relevant table (or rather function) 
that needs to be approximated because it depends on unknown transition probabilities 
and cannot be computed cheaply from the other quantities:

- min/maxAdmissibleQ and messingPotential_action can in principle be learned just as in DQN. 

- Q, Q2, ..., Q6, Q_DeltaSquare, Q_ones and LRAdev/behaviorEntropy/behaviorKLdiv_action
  can also be approximated by suitable ANNs,
  even though they have aleph (and sometimes p(a)) as additional input.

All these fulfil a Bellman equation, so they can be learned by a temporal difference learning approach 
with a suitable learning target (e.g., "expected SARSA" targets using the current policy based on the current estimates).
The other functions are cheap to compute and can be computed from the other quantities.

The theory overleaf contains a detailed proposal for the ANN architecture to use.


Policy-gradient approach
--------------------------

An alternative (but much less clear) approach is to use a policy gradient method,
where not Q but the policy directly is represented by an ANN.
*/


var _W = webpplAgents;

var makeMDPAgentSatisfia = function(params_, world) {

    // uncomment later once we want to do anything with the metalog distribution:
    // var metalog = getMetalog(); // needed to enable metalog distribution used in loss function
    
    var stateActionPairsSet = _W.emptySet(); // to be able to later loop over all state-action pairs visited

    // extend default parameters and options by supplied ones:
    var params = extend(extend({
      // admissibility parameters:
      maxLambda: 1, // upper bound on local relative aspiration in each step (must be minLambda...1)  // TODO: rename to lambdaHi
      minLambda: 0, // lower bound on local relative aspiration in each step (must be 0...maxLambda)  // TODO: rename to lambdaLo
      // policy parameters:
      lossTemperature: 0.1, // temperature of softmin mixture of actions w.r.t. loss, must be > 0
      rescaling4Actions: 0, // degree (0...1) of aspiration rescaling from state to action. (larger implies larger variance) // TODO: disable this because a value of >0 can't be taken into account in a consistent way easily
      rescaling4Successors: 1, // degree (0...1) of aspiration rescaling from action to successor state. (expectation is only preserved if this is 1.0) // TODO: disable also this since a value <1 leads to violation of the expectation guarantee 
      // coefficients for cheap to compute loss functions:
      lossCoeff4Random: 0, // weight of random tie breaker in loss function, must be >= 0
      lossCoeff4FeasibilityPower: 1, // weight of power of squared admissibility interval width in loss function, must be >= 0
      lossCoeff4MP: 1, // weight of messing potential in loss function, must be >= 0
      lossCoeff4LRA1: 1, // weight of current-state deviation of LRA from 0.5 in loss function, must be >= 0
      lossCoeff4Time1: 1, // weight of not terminating in loss function, must be >= 0
      lossCoeff4Entropy1: 1, // weight of current-state action entropy in loss function, must be >= 0
      lossCoeff4KLdiv1: 1, // weight of current-state KL divergence in loss function, must be >= 0
      // coefficients for expensive to compute loss functions (all zero by default except for variance):
      lossCoeff4Variance: 1, // weight of variance of total in loss function, must be >= 0
      lossCoeff4Fourth: 0, // weight of centralized fourth moment of total in loss function, must be >= 0
      lossCoeff4Cup: 0, // weight of "cup" loss component, based on sixth moment of total, must be >= 0
      lossCoeff4LRA: 0, // weight of deviation of LRA from 0.5 in loss function, must be >= 0
      lossCoeff4Time: 0, // weight of time in loss function, must be >= 0
      lossCoeff4DeltaVariation: 0, // weight of variation of Delta in loss function, must be >= 0
      lossCoeff4Entropy: 0, // weight of action entropy in loss function, must be >= 0
      lossCoeff4KLdiv: 0, // weight of KL divergence in loss function, must be >= 0
      lossCoeff4OtherLoss: 0, // weight of other loss components specified by otherLossIncrement, must be >= 0
      allowNegativeCoeffs: false, // if true, allow negative loss coefficients
    }, params_), { options: extend({
      verbose: false,  // if true, print explanatory messages
      debug: false  // if true, print extensive debugging messages
    }, params_.options) });
  
    // extract parameters, options, and world:
    var stateToActions = world.stateToActions,  // function(state)
        transition = world.transition;  // stochastic function(state, action)
    var expectedDelta = params.expectedDelta,  // function(state, action)
        varianceOfDelta = params.varianceOfDelta 
          || function(state, action) { return 0; },  // function(state, action)
        skewnessOfDelta = params.skewnessOfDelta
          || function(state, action) { return 0; },  // function(state, action)
        excessKurtosisOfDelta = params.excessKurtosisOfDelta
          || function(state, action) { return 0; },  // function(state, action)
        fifthMomentOfDelta = params.fifthMomentOfDelta
          || function(state, action) { 
            return 8*Math.pow(varianceOfDelta(state, action),2.5); // this assumes a Gaussian distribution
          },  // function(state, action)
        sixthMomentOfDelta = params.sixthMomentOfDelta
          || function(state, action) { 
            return 15*Math.pow(varianceOfDelta(state, action),3); // this assumes a Gaussian distribution
          },  // function(state, action)
        minLambda = params.minLambda, 
        maxLambda = params.maxLambda,
        lossTemperature = params.lossTemperature,
        rescaling4Actions = params.rescaling4Actions,
        rescaling4Successors = params.rescaling4Successors,
        lossCoeff4Random = params.lossCoeff4Random,
        lossCoeff4FeasibilityPower = params.lossCoeff4FeasibilityPower,
        lossCoeff4MP = params.lossCoeff4MP,
        uninformedStatePriorScore = params.uninformedStatePriorScore,  // function(state) used in messingPotential
        internalTransitionEntropy = params.internalTransitionEntropy,  // function(state, action, nextState) used in messingPotential
        lossCoeff4LRA1 = params.lossCoeff4LRA1,
        lossCoeff4Time1 = params.lossCoeff4Time1,
        lossCoeff4Entropy1 = params.lossCoeff4Entropy1,
        lossCoeff4KLdiv1 = params.lossCoeff4KLdiv1,
        lossCoeff4Variance = params.lossCoeff4Variance,
        lossCoeff4Fourth = params.lossCoeff4Fourth,
        lossCoeff4Cup = params.lossCoeff4Cup,
        lossCoeff4LRA = params.lossCoeff4LRA,
        lossCoeff4Time = params.lossCoeff4Time,
        lossCoeff4DeltaVariation = params.lossCoeff4DeltaVariation,
        lossCoeff4Entropy = params.lossCoeff4Entropy,
        uninformedPolicy = params.uninformedPolicy,  // function(state) used in entropy
        internalActionEntropy = params.internalActionEntropy,  // function(state, action) used in entropy
        lossCoeff4KLdiv = params.lossCoeff4KLdiv,
        referencePolicy = params.referencePolicy,  // function(state) used in KLdiv
        lossCoeff4OtherLoss = params.lossCoeff4OtherLoss,
        otherLocalLoss = params.otherLossIncrement,  // function(state, action, aleph, v, q, p)
        allowNegativeCoeffs = params.allowNegativeCoeffs;
    var options = params.options,
        verbose = options.verbose, 
        debug = options.debug;
  
    assert.ok(lossTemperature > 0, "lossTemperature must be > 0");
    assert.ok(0 <= rescaling4Actions <= 1, "rescaling4Actions must be in 0...1");
    assert.ok(0 <= rescaling4Successors <= 1, "rescaling4Successors must be in 0...1");
    assert.ok(allowNegativeCoeffs || lossCoeff4Random >= 0, "lossCoeff4random must be >= 0");
    assert.ok(allowNegativeCoeffs || lossCoeff4FeasibilityPower >= 0, "lossCoeff4FeasibilityPower must be >= 0");
    assert.ok(allowNegativeCoeffs || lossCoeff4MP >= 0, "lossCoeff4MP must be >= 0");
    assert.ok(allowNegativeCoeffs || lossCoeff4LRA1 >= 0, "lossCoeff4LRA1 must be >= 0");
    assert.ok(allowNegativeCoeffs || lossCoeff4Time1 >= 0, "lossCoeff4Time1 must be >= 0");
    assert.ok(allowNegativeCoeffs || lossCoeff4Entropy1 >= 0, "lossCoeff4Entropy1 must be >= 0");
    assert.ok(allowNegativeCoeffs || lossCoeff4KLdiv1 >= 0, "lossCoeff4KLdiv1 must be >= 0");
    assert.ok(allowNegativeCoeffs || lossCoeff4Variance >= 0, "lossCoeff4variance must be >= 0");
    assert.ok(allowNegativeCoeffs || lossCoeff4Fourth >= 0, "lossCoeff4Fourth must be >= 0");
    assert.ok(allowNegativeCoeffs || lossCoeff4Cup >= 0, "lossCoeff4Cup must be >= 0");
    assert.ok(allowNegativeCoeffs || lossCoeff4LRA >= 0, "lossCoeff4LRA must be >= 0");
    assert.ok(allowNegativeCoeffs || lossCoeff4Time >= 0, "lossCoeff4time must be >= 0");
    assert.ok(allowNegativeCoeffs || lossCoeff4DeltaVariation >= 0, "lossCoeff4DeltaVariation must be >= 0");
    assert.ok(allowNegativeCoeffs || lossCoeff4Entropy >= 0, "lossCoeff4entropy must be >= 0");
    assert.ok(allowNegativeCoeffs || lossCoeff4KLdiv >= 0, "lossCoeff4KLdiv must be >= 0");
    assert.ok(allowNegativeCoeffs || lossCoeff4OtherLoss >= 0, "lossCoeff4OtherLoss must be >= 0");
    assert.ok(lossCoeff4Entropy == 0 || lossCoeff4MP == 0 || uninformedPolicy != undefined, "uninformedPolicy must be provided if lossCoeff4MP > 0 or lossCoeff4Entropy > 0");
  
    if (verbose || debug) console.log("makeMDPAgentSatisfia with parameters", params);
  
    /*  The dependency/callback graph of the following functions is partially recursive 
        and involves aggregation (MIN, MAX, E) operations as follows:
  
        simulate
        → localPolicy
          → aspiration4state
            → minAdmissibleV, maxAdmissibleV
              → MIN(minAdmissibleQ), MAX(maxAdmissibleQ)
                → E(minAdmissibleV), E(maxAdmissibleV) (RECURSION)
          → estAspiration4action
            → minAdmissibleV, maxAdmissibleV, minAdmissibleQ, maxAdmissibleQ
          → combinedLoss
            → Q, .., Q6, Q_DeltaSquare, Q_ones
              → propagateAspiration (see below)
              → E(V), ..., E(V6), E(V_DeltaSquare), E(V_ones)
                → localPolicy (RECURSION)
                → E(Q), ..., E(Q6), E(Q_DeltaSquare), E(Q_ones) (RECURSION)
            → otherLoss_action
              → propagateAspiration (see below)
              → E(otherLoss_state)
                → localPolicy (RECURSION)
                → E(otherLoss_action) (RECURSION)
            → similarly for other loss components (RECURSION)
        → expectedDelta, varianceOfDelta, transition
        → propagateAspiration
          → aspiration4state
        → simulate (RECURSION)
    */


    // Utility function for deriving transition probabilities from the transition function:
    // Remark: later the following should actually be provided by the env/world:

    var transitionDistribution = dp.cache(function(state, action) {
      return Infer({ model() {return transition(state, action); }});
    });


    // Compute upper and lower admissibility bounds for Q and V that are allowed in view of maxLambda and minLambda:
  
    // Compute the Q and V functions of the classical maximization problem (if maxLambda==1)
    // or of the LRA-based problem (if maxLambda<1):
    // ! recursive function:
    var maxAdmissibleQ = dp.cache(function(state, action){
      if (verbose || debug) console.log(pad(state),"| | | maxAdmissibleQ, state",prettyState(state),"action",action,"...");
      stateActionPairsSet.add([state,action]); // register (state, action) in global store (could be anywhere, but here is just as fine as anywhere else)
      var Edel = expectedDelta(state, action),
          q = state.terminateAfterAction 
              ? Edel 
              : Edel + expectation(Infer({ model() { 
                return maxAdmissibleV(transition(state, action));  // ! recursion here ! 
              }}));  // Bellman equation
      if (verbose || debug) console.log(pad(state),"| | | maxAdmissibleQ, state",prettyState(state),"action",action,":",q);
      return q;
    });
    // ! recursive function:
    var maxAdmissibleV = dp.cache(function(state){
      if (verbose || debug) console.log(pad(state),"| | | maxAdmissibleV, state",prettyState(state),"...");
      var actions = stateToActions(state),
          qs = map(function(a) { return maxAdmissibleQ(state, a); }, actions),  // ! recursion here !
          v = maxLambda == 1 
              ? _W.max(qs) 
              : interpolate(_W.min(qs), maxLambda, _W.max(qs));
      if (verbose || debug) console.log(pad(state),"| | | maxAdmissibleV, state",prettyState(state),":",v);
      return v;
    });
  
    // Compute the Q and V functions of the corresponding minimization (!) problem (if minLambda==0)
    // or of the LRA-based problem (if minLambda>0):
    // ! recursive function:
    var minAdmissibleQ = dp.cache(function(state, action){
      if (verbose || debug) console.log(pad(state),"| | | minAdmissibleQ, state",prettyState(state),"action",action,"...");
      var Edel = expectedDelta(state, action),
          q = state.terminateAfterAction 
              ? Edel 
              : Edel + expectation(Infer({ model() { 
                return minAdmissibleV(transition(state, action));  // ! recursion here !
              }}));
      if (verbose || debug) console.log(pad(state),"| | | minAdmissibleQ, state",prettyState(state),"action",action,":",q);
      return q;
    });
    // ! recursive function:
    var minAdmissibleV = dp.cache(function(state){
      if (verbose || debug) console.log(pad(state),"| | | minAdmissibleV, state",prettyState(state),"...");
      var actions = stateToActions(state);
      var qs = map(function(action) { return minAdmissibleQ(state, action); }, actions),  // ! recursion here !
          v = minLambda == 0 
              ? _W.min(qs)
              : interpolate(_W.min(qs), minLambda, _W.max(qs));
      if (verbose || debug) console.log(pad(state),"| | | minAdmissibleV, state",prettyState(state),":",v);
      return v;
    });

    // The resulting admissibility intervals for states and actions:
    var admissibility4state = dp.cache(function(state){
      return [minAdmissibleV(state), maxAdmissibleV(state)];
    });
    var admissibility4action = dp.cache(function(state, action){
      return [minAdmissibleQ(state, action), maxAdmissibleQ(state, action)];
    });

    // When in state, we can get any expected total in the interval
    // [minAdmissibleV(state), maxAdmissibleV(state)].
    // So when having aspiration aleph, we can still fulfill it in expectation if it lies in the interval.
    // Therefore, when in state at incoming aspiration aleph, 
    // we adjust our aspiration to aleph clipped to that interval:
    var aspiration4state = dp.cache(function(state, unclippedAleph){
      if (verbose || debug) console.log(pad(state),"| | aspiration4state, state",prettyState(state),"unclippedAleph",unclippedAleph,"...");
      var res = clip(minAdmissibleV(state), asInterval(unclippedAleph), maxAdmissibleV(state));
      if (verbose || debug) console.log(pad(state),"| | aspiration4state, state",prettyState(state),"unclippedAleph",unclippedAleph,":",res);
      return res;
    });
  
    // When constructing the local policy, we first use an estimated action aspiration interval
    // that does not depend on the local policy but is simply based on the state's aspiration interval,
    // rescaled from the admissibility interval of the state to the admissibility interval of the action.
    var estAspiration4action = dp.cache(function(state, action, aleph4state){
      if (/*verbose ||*/ debug) console.log("| | estAspiration4action, state",prettyState(state),"action",action,"aleph4state",aleph4state,"...");
      var phi = admissibility4action(state, action);
      if (isSubsetOf(phi, aleph4state)) {

        if (verbose || debug) console.log(pad(state),"| | estAspiration4action, state",prettyState(state),"action",action,"aleph4state",aleph4state,":",phi,"(subset of aleph4state)");
        return phi;

      } else {
        // if rescaling4Actions == 1, we use a completely rescaled version:
        var rescaled = interpolate(minAdmissibleQ(state, action), 
                                   relativePosition(minAdmissibleV(state), aleph4state, maxAdmissibleV(state)),
                                   maxAdmissibleQ(state, action));
        // if rescaling4Actions == 0, we use a steadfast version that does make sure that aleph(a) is no wider than aleph(s):
        // - If phi(a) contains aleph(s), then aleph(a) = aleph(s)
        // - If aleph(s) contains phi(a), then aleph(a) = phi(a)
        // - If phiLo(a) < alephLo(s) and phiHi(a) < alephHi(s), then aleph(a) = [max(phiLo(a), phiHi(a) - alephW(s)), phiHi(a)]
        // - If phiHi(a) > alephHi(s) and phiLo(a) > alephLo(s), then aleph(a) = [phiLo(a), min(phiHi(a), phiLo(a) + alephW(s))]
        var phiLo = phi[0], phiHi = phi[1], alephLo = aleph4state[0], alephHi = aleph4state[1], w = alephHi - alephLo,
            steadfast = isSubsetOf(aleph4state, phi) ? aleph4state 
                      : (phiLo < alephLo && phiHi < alephHi) ? [Math.max(phiLo, phiHi - w), phiHi] 
                      : (phiHi > alephHi && phiLo > alephLo) ? [phiLo, Math.min(phiHi, phiLo + w)] 
                      : phi;

        // We interpolate between the two versions according to rescaling4Actions:
        var res = interpolate(steadfast, rescaling4Actions, rescaled);

        if (verbose || debug) console.log(pad(state),"| | estAspiration4action, state",prettyState(state),"action",action,"aleph4state",aleph4state,":",res,"(steadfast/rescaled)");
        return res;
      }
    });
    // TODO: Consider two other alternatives:
    // 1. Only rescale the width and not the location of the aspiration interval, 
    // and move it as close as possible to the state aspiration interval
    // (but maybe keeping a minimal safety distance from the bounds of the admissibility interval of the action).
    // In both cases, if the admissibility interval of the action is larger than that of the state, 
    // the final action aspiration interval might need to be shrinked to fit into the aspiration interval of the state
    // once the mixture is know.
    // 2. This could be avoided by a further modification, where we rescale only downwards, never upwards:
    // - If phi(a) contains aleph(s), then aleph(a) = aleph(s)
    // - If aleph(s) contains phi(a), then aleph(a) = phiMid(a) +- alephW(s)*phiW(a)/phiW(s) / 2
    // - If phiLo(a) < alephLo(s) and phiHi(a) < alephHi(s), then aleph(a) = phiHi(a) - [0, alephW(s)*min(1,phiW(a)/phiW(s))]
    // - If phiHi(a) > alephHi(s) and phiLo(a) > alephLo(s), then aleph(a) = phiLo(a) + [0, alephW(s)*min(1,phiW(a)/phiW(s))]


    // Some safety metrics do not depend on aspiration and can thus also be computed upfront,
    // like min/maxAdmissibleQ, min/maxAdmissibleV:

    // Messing potential (maximal entropy (relative to some uninformedStatePrior) 
    // over trajectories any agent could produce from here (see overleaf for details)):
    // recursive function:
    var messingPotential_action = dp.cache(function(state, action) {
      // Note for ANN approximation: messingPotential_action can be positive or negative. 
      var MP = expectation(Infer({ model() {
            if (state.terminateAfterAction){
              return 0;
            } else {
              var nextState = transition(state, action),
                  nextStateScore = transitionDistribution(state, action).score(nextState),
                  nextMP = messingPotential_state(nextState); // ! recursion here !
              return nextMP + (uninformedStatePriorScore ? uninformedStatePriorScore(state) : 0) - nextStateScore
                     + (internalTransitionEntropy ? internalTransitionEntropy(state, action, nextState) : 0);
            }
          }}));
      return MP;
    });
    // recursive function:
    var messingPotential_state = dp.cache(function(state) {
      var actions = stateToActions(state),
          maxMPpolicyWeights = map(function(a) { return Math.exp(messingPotential_action(state, a)); }, actions), 
          MP = Math.log(sum(maxMPpolicyWeights));
      return MP;
    });


    // Based on the admissibility information computed above, we can now construct the policy,
    // which is a mapping taking a state and an aspiration interval as input and returning
    // a categorical distribution over (action, aleph4action) pairs.

    // ! recursive function:
    var localPolicy = function(state, aleph) {
      // return a categorical distribution over (action, aleph4action) pairs
      var d = localPolicyData(state, aleph),
          support = d[0], ps = d[1];
      if (debug) console.log(" localPolicy", prettyState(state), aleph, d);
      return Categorical({vs: support, ps: ps});
    }
    var localPolicyData = dp.cache(function(state, aleph){
      if (verbose || debug) console.log(pad(state),"| localPolicy, state",prettyState(state),"aleph",aleph,"...");
      
      // Clip aspiration interval to admissibility interval of state:
      var aleph4state = aspiration4state(state, aleph),
          alephLo = aleph4state[0], alephHi = aleph4state[1];

      // Estimate aspiration intervals for all possible actions in a way 
      // independent from the local policy that we are about to construct,
      var actions = stateToActions(state),
          estAlephs1 = map(function(action) { return estAspiration4action(state, action, aleph4state); }, actions);

      // Estimate losses based on this estimated aspiration intervals
      // and use it to construct softmin propensities (probability weights) for choosing actions.
      // since we don't know the actual probabilities of actions yet (we will determine those only later),
      // but the loss estimation requires an estimate of the probability of the chosen action,
      // we estimate the probability at 1 / number of actions:
      var indices = Array.from(actions.keys()),
          propensities = map(function(index) { 
            var action = actions[index],
                loss = combinedLoss(state, action, aleph4state, estAlephs1[index], 1 / indices.length);  // ! bottleneck !
            return Math.min(1e100, Math.max(Math.exp(-loss / lossTemperature), 1e-100)); 
          }, indices);

      if (debug) console.log("| localPolicyData", prettyState(state), aleph, actions, {propensities});

      // now we can construct the local policy as a WebPPL distribution object:
      var locPol = Infer({ model() {

        // Draw a first action a1 using the calculated softmin propensities,
        // and get its admissibility interval:
        var i1 = sample(Categorical({vs: indices, ps: propensities})),
            a1 = actions[i1],
            adm1 = admissibility4action(state, a1),
            adm1Lo = adm1[0], adm1Hi = adm1[1];

        // If a1's admissibility interval is completely contained in aleph4state, we are done:
        if (isSubsetOf(adm1, aleph4state)) {

          if (verbose || debug) console.log(pad(state),"| | locPol, state",prettyState(state),"aleph4state",aleph4state,": a1",a1,"adm1",adm1,"(subset of aleph4state)");
          return [a1, adm1];

        } else {

          // For drawing the second action, restrict actions so that the the midpoint of aleph4state can be mixed from
          // those of estAlephs4action of the first and second action:
          var midTarget = midpoint(aleph4state),
              estAleph1 = estAlephs1[i1],
              mid1 = midpoint(estAleph1),
              indices2 = (mid1 <= midTarget) 
                          ? filter(function(index) { return midpoint(estAlephs1[index]) >= midTarget; }, indices)
                          : filter(function(index) { return midpoint(estAlephs1[index]) <= midTarget; }, indices);
          // Since we are already set on giving a1 a considerable weight, we no longer aim to have aleph(a2)
          // as close as possible to aleph(s), but to a target aleph that would allow mixing a1 and a2 
          // in roughly equal proportions, i.e., we aim to have aleph(a2) as close as possible to
          // aleph(s) + (aleph(s) - aleph(a1)):
          var aleph2target = interpolate(estAleph1, 2.0, aleph4state);
          // Due to the new target aleph, we have to recompute the estimated alephs and resulting losses and propensities:
          var estAlephs2 = map(function(index) { return estAspiration4action(state, actions[index], aleph2target); }, indices),
              propensities2 = map(function(index) { 
                var action = actions[index],
                    loss = combinedLoss(state, action, aleph4state, estAlephs2[index], 1 / indices2.length);
                return Math.min(1e100, Math.max(Math.exp(-loss / lossTemperature), 1e-100)); 
              }, indices2);

          if (debug) console.log("| localPolicyData", prettyState(state), aleph4state, {a1, midTarget, estAleph1, mid1, indices2, aleph2target, estAlephs2, propensities2});

          // Like for a1, we now draw a2 using a softmin mixture of these actions, based on the new propentities,
          // and get its admissibility interval:
          var i2 = sample(Categorical({vs: indices2, ps: propensities2})),
              a2 = actions[i2], 
              adm2 = admissibility4action(state, a2),
              adm2Lo = adm2[0], adm2Hi = adm2[1];

          // Now we need to find two aspiration intervals aleph1 in adm1 and aleph2 in adm2, 
          // and a probability p such that
          //   aleph1:p:aleph2 is contained in aleph4state 
          // and aleph1, aleph2 are close to the estimates we used above in estimating loss.
          // Instead of optimizing this, we use the following heuristic:
          // We first choose p so that the midpoints mix exactly:
          var estAleph2 = estAlephs2[i2],
              mid2 = midpoint(estAleph2),
              p = relativePosition(mid1, midTarget, mid2);

          // Now we find the largest relative size of aleph1 and aleph2 
          // so that their mixture is still contained in aleph4state:
          // we want aleph1Lo:p:aleph2Lo >= alephLo and aleph1Hi:p:aleph2Hi <= alephHi
          // where aleph1Lo = mid1 - x * w1, aleph1Hi = mid1 + x * w1, 
          //       aleph2Lo = mid2 - x * w2, aleph2Hi = mid2 + x * w2,
          // hence midTarget - x * w1:p:w2 >= alephLo and midTarget + x * w1:p:w2 <= alephHi,
          // i.e., x <= (midTarget - alephLo) / (w1:p:w2) and x <= (alephHi - midTarget) / (w1:p:w2): 
          var w1 = estAleph1[1] - estAleph1[0], 
              w2 = estAleph2[1] - estAleph2[0],
              w = interpolate(w1, p, w2),
              x = w > 0 ? Math.min((midTarget - alephLo) / w, (alephHi - midTarget) / w) : 0,
              aleph1 = [mid1 - x * w1, mid1 + x * w1],
              aleph2 = [mid2 - x * w2, mid2 + x * w2];
                
          if (debug) console.log("| localPolicyData",prettyState(state), aleph4state, {a1, estAleph1, adm1: adm1, w1, a2, estAleph2, adm2, w2, p, w, x, aleph1, aleph2});

          if (verbose || debug) console.log(pad(state),"| | locPol, state",prettyState(state),"aleph4state",aleph4state,": a1,p,a2",a1,p,a2,"adm12",adm1,adm2,"aleph12",aleph1,aleph2);

          return sample(Categorical({vs: [[a1, aleph1], [a2, aleph2]], ps: [1-p, p]}));

          // TODO: understand why in GW3 we go right with 50% probability rather than always going left.
        }

        if (false) {  // FIXME: this optimization version would be better but it doesn't work yet:
  
          // For drawing the second action, possibly restrict actions so that the union of the convex hull of the two admissibility intervals covers aleph4state:
          var indices2 = isSubsetOf(aleph4state, adm1) 
                        ? indices  // still all actions are feasible
                        : ( // only actions "on the other side" are feasible
                            (adm1Lo <= alephLo) 
                            ? filter(function(index) { return maxAdmissibleQ(state, actions[index]) >= alephHi; }, indices)
                            : filter(function(index) { return minAdmissibleQ(state, actions[index]) <= alephLo; }, indices)
                          ),
              propensities2 = map(function(index) { return propensities[index]; }, indices2);
          // Draw a second action using a softmin mixture of these actions, based on the loss,
          // and get its admissibility interval:
          var i2 = sample(Categorical({vs: indices2, ps: propensities2})),
              a2 = actions[i2], 
              adm2 = admissibility4action(state, a2),
              adm2Lo = adm2[0], adm2Hi = adm2[1];

          if (debug) console.log("|  ", prettyState(state), aleph, indices2, a2, adm2, adm2Lo, adm2Hi);

          // If a2's admissibility interval is completely contained in aleph4state, we are done:
          if (isSubsetOf(adm2, aleph4state)) {

            return [a2, aleph4state];
    
          } else {
    
            // Now we need to find two aspiration intervals aleph1 in adm1 and aleph2 in adm2, 
            // and a probability p such that
            //   aleph1:p:aleph2 = aleph4state 
            // and aleph1, aleph2 are close to the estimates we used above in estimating loss.
            // To measure the error, we use the sum of the squared deviations of aleph1, aleph2 from the estimates,
            // divided by the squared width of adm1 and adm2.
            var estAleph1 = estAlephs1[i1], 
                estAleph2 = estAlephs1[i2], 
                target1Lo = estAleph1[0], target1Hi = estAleph1[1], wsq1 = squared(adm1Hi - adm1Lo),
                target2Lo = estAleph2[0], target2Hi = estAleph2[1], wsq2 = squared(adm2Hi - adm2Lo);

            if (wsq1 == 0 && wsq2 == 0) {

              var aleph1 = adm1, aleph2 = adm2, p = relativePosition(adm1Lo, midpoint(aleph4state), adm2Lo); 
              return sample(Categorical({vs: [[a1, aleph1], [a2, aleph2]], ps: [1-p, p]}));

            } else {

              // Solving this constrained optimization problem by setting the gradient of the squared deviation
              // w.r.t. to aleph1, aleph2, and p to zero yields:
              var d1Lo = target1Lo - alephLo, d1Hi = target1Hi - alephHi,
                  d2Lo = target2Lo - alephLo, d2Hi = target2Hi - alephHi,
                  facLo = d1Lo * wsq2 - d2Lo * wsq1,
                  facHi = d1Hi * wsq2 - d2Hi * wsq1,
                  p = clip(0, (d1Lo*facLo + d1Hi*facHi) / ((adm1Lo-adm2Lo)*facLo + (adm1Hi-adm2Hi)*facHi), 1),
                  wsq = interpolate(wsq1, p, wsq2),
                  devLo = (alephLo - interpolate(target1Lo, p, target2Lo)) / wsq,
                  devHi = (alephHi - interpolate(target1Hi, p, target2Hi)) / wsq,
                  aleph1 = [target1Lo + devLo * wsq1, target1Hi + devHi * wsq1],
                  aleph2 = [target2Lo + devLo * wsq2, target2Hi + devHi * wsq2];
                  
              if (debug) console.log("|  ",prettyState(state), aleph4state,
                "\n    ", a1, estAleph1, adm1, wsq1, d1Lo, d1Hi, 
                "\n    ", a2, estAleph2, adm2, wsq2, d2Lo, d2Hi, 
                "\n    ", facLo, facHi, p, wsq, devLo, devHi, 
                "\n    ", aleph1, aleph2);
              // TODO: verify that the following inquality constraints are always satisfied, otherwise deal with it:
              assert.ok(0 <= p && p <= 1, "OOPS, infeasible optimal p: " + p);
              assert.ok(isSubsetOf(aleph1, adm1), "OOPS, infeasible aleph1: " + aleph1 + " not in " + adm1);
              assert.ok(isSubsetOf(aleph2, adm2), "OOPS, infeasible aleph2: " + aleph2 + " not in " + adm2);

              return sample(Categorical({vs: [[a1, aleph1], [a2, aleph2]], ps: [1-p, p]}));
            }
          }
        }
      }});

      var support = locPol.support(),
          ps = map(function(item) { 
                    return Math.max(1e-100, Math.exp(locPol.score(item)));  // 1e-100 prevents normalization problems
                  }, support);

      if (verbose || debug) {
        console.log(pad(state),"| localPolicy, state",prettyState(state),"aleph",aleph,":");
        _W.printPolicy(pad(state), support, ps);
      }
      return [support, ps];

    });
  
    // Propagate aspiration from state-action to successor state, potentially taking into account received expected delta:

    var propagateAspiration = // dp.cache(  // caching this easy to compute function would only clutter the cache due to its many arguments
      function(state, action, aleph4action, Edel, nextState){  

        if (/*verbose ||*/ debug) console.log(pad(state),"| | | propagateAspiration, state",prettyState(state),"action",action,"aleph4action",aleph4action,"Edel",Edel,"nextState",prettyState(nextState),"...");

        // the propagated aspiration is a mixture of a steadfast and a rescaled part:
    
        var steadfastAleph4nextState = [aleph4action[0] - Edel, aleph4action[1] - Edel];
    
        if (rescaling4Successors == 0.0) {

          var res = steadfastAleph4nextState;
          if (verbose || debug) console.log(pad(state),"| | | propagateAspiration, state",prettyState(state),"action",action,"aleph4action",aleph4action,"Edel",Edel,"nextState",prettyState(nextState),":",res);
          return res;

        } else {

          // compute the relative position of aleph4action in the expectation that we had of 
          //    delta + next admissibility interval 
          // before we knew which state we would land in:
          var lam = relativePosition(minAdmissibleQ(state, action), aleph4action, maxAdmissibleQ(state, action)); 
          // (this is two numbers between 0 and 1.)
          // use it to rescale aleph4action to the admissibility interval of the state that we landed in:
          var rescaledAleph4nextState = interpolate(minAdmissibleV(nextState), lam, maxAdmissibleV(nextState));
          // (only this part preserves aspiration in expectation)
          var res = interpolate(steadfastAleph4nextState, rescaling4Successors, rescaledAleph4nextState);      
          if (verbose || debug) console.log(pad(state),"| | | propagateAspiration, state",prettyState(state),"action",action,"aleph4action",aleph4action,"Edel",Edel,"nextState",prettyState(nextState),":",res);
          return res;

          /* Note on influence of Edel: 
          It might seem that the (expected) delta received when taking action a in state s should occur
          explicitly in some form in this formula, similar to how it occurred in the steadfast formula above.
          This is not so, however. The expected delta is taken account of *implicitly* in the rescaling formula
          via the use of min/maxAdmissibleQ(s,a) to compute lam but using min/maxAdmissibleV(s') in interpolating.
          More precisely, one can prove that aspirations are kept in expectation. We want

            aleph(s,a) = E(delta(s,a)) + E(aleph(s') | s'~(s,a)).
          
          This can be shown to be true as follows:
          
            min/maxAdmissibleQ(s,a) = E(delta(s,a)) + E(min/maxAdmissibleV(s') | s'~(s,a)),
            
            lamdba = (aleph(s,a) - minAdmissibleQ(s,a)) / (maxAdmissibleQ(s,a) - minAdmissibleQ(s,a)),
            
            rescaledAleph(s') = minAdmissibleV(s') + lambda * (maxAdmissibleV(s') - minAdmissibleV(s')),
            
            E(delta(s,a)) + E(rescaledAleph(s') | s'~(s,a)) 
            = E(delta(s,a)) + E(minAdmissibleV(s') | s'~(s,a)) 
              + lambda * (E(maxAdmissibleV(s') | s'~(s,a)) - E(minAdmissibleV(s') | s'~(s,a)))
            = minAdmissibleQ(s,a) + lambda * (maxAdmissibleQ(s,a) - minAdmissibleQ(s,a))
            = minAdmissibleQ(s,a) + (aleph(s,a) - minAdmissibleQ(s,a))
            = aleph(s,a).

          So the above rescaling formula is correctly taking account of received delta even without explicitly
          including Edel in the formula.
          */
        }
      }
    // )
    ;
  
    // Based on the policy, we can compute many resulting quantities of interest useful in assessing safety
    // better than with the above myopic safety metrics. All of them satisfy Bellman-style equations:

    // Actual Q and V functions of resulting policy (always returning scalars):
    // ! recursive function:
    var Q = dp.cache(function(state, action, aleph4action){
      if (debug) console.log("| Q", prettyState(state), action, aleph4action);
      var q = expectation(Infer({ model() {
        var Edel = expectedDelta(state, action);
        if (state.terminateAfterAction){
          return Edel;
        } else {
          var nextState = transition(state, action),
              nextAleph4state = propagateAspiration(state, action, aleph4action, Edel, nextState);
          return Edel + V(nextState, nextAleph4state);  // ! recursion here !
        }
      }})); 
      if (debug) console.log("| Q", prettyState(state), action, aleph4action, q);
      return q;
    });
    // ! recursive function:
    var V = dp.cache(function(state, aleph4state){
      if (debug) console.log("| V", prettyState(state), aleph4state);
      var locPol = localPolicy(state, aleph4state);
      var v = expectation(Infer({ model() {
        var actionAndAleph = sample(locPol);  // ! recursion here !
        return Q(state, actionAndAleph[0], actionAndAleph[1]);  // ! recursion here !
      }}));
      if (debug) console.log("| V", prettyState(state), aleph4state, ":", v);
      return v;
    });
  
    // Raw moments of delta: 
    var expectedDeltaSquared = dp.cache(function(state, action){
      var Edel = expectedDelta(state, action);
      return varianceOfDelta(state, action) + squared(Edel);
    });
    var expectedDeltaCubed = dp.cache(function(state, action){
      var Edel = expectedDelta(state, action),
          varDel = varianceOfDelta(state, action);
      return Math.pow(varDel, 1.5)*skewnessOfDelta(state, action) 
             + 3*Edel*varDel 
             + Math.pow(Edel, 3);
    });
    var expectedDeltaFourth = dp.cache(function(s, a){
      var Edel = expectedDelta(s, a);
      return squared(varianceOfDelta(s, a)) * (3 + excessKurtosisOfDelta(s, a))
             + 4*expectedDeltaCubed(s, a)*Edel 
             - 6*expectedDeltaSquared(s, a)*squared(Edel) 
             + 3*Math.pow(Edel, 4);
    });
    var expectedDeltaFifth= dp.cache(function(s, a){
      var Edel = expectedDelta(s, a);
      return fifthMomentOfDelta(s, a)
             + 5*expectedDeltaFourth(s, a)*Edel 
             - 10*expectedDeltaCubed(s, a)*squared(Edel) 
             + 10*expectedDeltaSquared(s, a)*cubed(Edel) 
             - 4*Math.pow(Edel, 5);
    });
    var expectedDeltaSixth = dp.cache(function(s, a){
      var Edel = expectedDelta(s, a);
      return sixthMomentOfDelta(s, a)
             + 6*expectedDeltaFifth(s, a)*Edel 
             - 15*expectedDeltaFourth(s, a)*squared(Edel) 
             + 20*expectedDeltaCubed(s, a)*cubed(Edel) 
             - 15*expectedDeltaSquared(s, a)*Math.pow(Edel, 4) 
             + 5*Math.pow(Edel, 6);
    });
  
    // Expected squared total, for computing the variance of total:
    // ! recursive function:
    var Q2 = dp.cache(function(state, action, aleph4action){
      var q2 = expectation(Infer({ model() {
        var Edel = expectedDelta(state, action),
            Edel2 = expectedDeltaSquared(state, action);
        if (state.terminateAfterAction){
          return Edel2;
        } else {
          var nextState = transition(state, action),
              nextAleph4state = propagateAspiration(state, action, aleph4action, Edel, nextState);
          // TODO: verify formula:
          return Edel2 
                 + 2*Edel*V(nextState, nextAleph4state) 
                 + V2(nextState, nextAleph4state); // ! recursion here !
        }
      }}));
      if (debug) console.log("| Q2", prettyState(state), action, aleph4action, q2);
      return q2;
    });
    // ! recursive function:
    var V2 = dp.cache(function(state, aleph4state){
      var locPol = localPolicy(state, aleph4state);
      var v2 = expectation(Infer({ model() { 
        var actionAndAleph = sample(locPol);  // ! recursion here !
        return Q2(state, actionAndAleph[0], actionAndAleph[1]);  // ! recursion here !
      }}));
      if (debug) console.log("| V2", prettyState(state), aleph4state, v2);
      return v2;
    });
  
    // Similarly: Expected third and fourth powers of total, for computing the 3rd and 4th centralized moment of total:
    // ! recursive function:
    var Q3 = dp.cache(function(state, action, aleph4action){
      var q3 = expectation(Infer({ model() {
        var Edel = expectedDelta(state, action),
            Edel2 = expectedDeltaSquared(state, action),
            Edel3 = expectedDeltaCubed(state, action);
        if (state.terminateAfterAction){
          return Edel3;
        } else {
          var nextState = transition(state, action),
              nextAleph4state = propagateAspiration(state, action, aleph4action, Edel, nextState);
          // TODO: verify formula:
          return Edel3 
                 + 3*Edel2*V(nextState, nextAleph4state) 
                 + 3*Edel*V2(nextState, nextAleph4state) 
                 + V3(nextState, nextAleph4state); // ! recursion here !
        }
      }}));
      if (debug) console.log("| Q3", prettyState(state), action, aleph4action, q3);
      return q3;
    });
    // ! recursive function:
    var V3 = dp.cache(function(state, aleph4state){
      var locPol = localPolicy(state, aleph4state);
      var v3 = expectation(Infer({ model() {
        var actionAndAleph = sample(locPol);  // ! recursion here !
        return Q3(state, actionAndAleph[0], actionAndAleph[1]);  // ! recursion here !
      }}));
      if (debug) console.log("| V3", prettyState(state), aleph4state, v3);
      return v3;
    });
  
    // Expected fourth power of total, for computing the expected fourth power of deviation of total from expected total (= fourth centralized moment of total):
    // ! recursive function:
    var Q4 = dp.cache(function(state, action, aleph4action){
      var q4 = expectation(Infer({ model() {
        var Edel = expectedDelta(state, action),
            Edel2 = expectedDeltaSquared(state, action),
            Edel3 = expectedDeltaCubed(state, action),
            Edel4 = expectedDeltaFourth(state, action);
        if (state.terminateAfterAction){
          return Edel4;
        } else {
          var nextState = transition(state, action),
              nextAleph4state = propagateAspiration(state, action, aleph4action, Edel, nextState);
          // TODO: verify formula:
          return Edel4 
                 + 4*Edel3*V(nextState, nextAleph4state) 
                 + 6*Edel2*V2(nextState, nextAleph4state) 
                 + 4*Edel*V3(nextState, nextAleph4state) 
                 + V4(nextState, nextAleph4state); // ! recursion here !
        }
      }}));
      if (debug) console.log("| Q4", prettyState(state), action, aleph4action, q4);
      return q4;
    });
    // ! recursive function:
    var V4 = dp.cache(function(state, aleph4state){
      var locPol = localPolicy(state, aleph4state);
      var v4 = expectation(Infer({ model() {
        var actionAndAleph = sample(locPol);  // ! recursion here !
        return Q4(state, actionAndAleph[0], actionAndAleph[1]);  // ! recursion here !
      }}));
      if (debug) console.log("| V4", prettyState(state), aleph4state, v4);
      return v4;
    });
  
    // Expected fifth power of total, for computing the bed-and-banks loss component based on a 6th order polynomial potential of this shape: https://www.wolframalpha.com/input?i=plot+%28x%2B1%29%C2%B3%28x-1%29%C2%B3+ :
    // ! recursive function:
    var Q5 = dp.cache(function(state, action, aleph4action){
      var q5 = expectation(Infer({ model() {
        var Edel = expectedDelta(state, action),
            Edel2 = expectedDeltaSquared(state, action),
            Edel3 = expectedDeltaCubed(state, action),
            Edel4 = expectedDeltaFourth(state, action),
            Edel5 = expectedDeltaFifth(state, action);
        if (state.terminateAfterAction){
          return Edel5;
        } else {
          var nextState = transition(state, action),
              nextAleph4state = propagateAspiration(state, action, aleph4action, Edel, nextState);
          // TODO: verify formula:
          return Edel5 
                 + 5*Edel4*V(nextState, nextAleph4state) 
                 + 10*Edel3*V2(nextState, nextAleph4state) 
                 + 10*Edel2*V3(nextState, nextAleph4state) 
                 + 5*Edel*V4(nextState, nextAleph4state) 
                 + V5(nextState, nextAleph4state); // ! recursion here !
        }
      }}));
      if (debug) console.log("| Q5", prettyState(state), action, aleph4action, q5);
      return q5;
    });
    // ! recursive function:
    var V5 = dp.cache(function(state, aleph4state){
      var locPol = localPolicy(state, aleph4state);
      var v5 = expectation(Infer({ model() {
        var actionAndAleph = sample(locPol);  // ! recursion here !
        return Q5(state, actionAndAleph[0], actionAndAleph[1]);  // ! recursion here !
      }}));
      if (debug) console.log("| V5", prettyState(state), aleph4state, v5);
      return v5;
    });
    
    // Expected sixth power of total, for computing the bed-and-banks loss component based on a 6th order polynomial potential of this shape: https://www.wolframalpha.com/input?i=plot+%28x%2B1%29%C2%B3%28x-1%29%C2%B3+ :
    // ! recursive function:
    var Q6 = dp.cache(function(state, action, aleph4action){
      var q6 = expectation(Infer({ model() {
        var Edel = expectedDelta(state, action),
            Edel2 = expectedDeltaSquared(state, action),
            Edel3 = expectedDeltaCubed(state, action),
            Edel4 = expectedDeltaFourth(state, action),
            Edel5 = expectedDeltaFifth(state, action),
            Edel6 = expectedDeltaSixth(state, action);
        if (state.terminateAfterAction){
          return Edel6;
        } else {
          var nextState = transition(state, action),
              nextAleph4state = propagateAspiration(state, action, aleph4action, Edel, nextState);
          // TODO: verify formula:
          return Edel6 
                 + 6*Edel5*V(nextState, nextAleph4state) 
                 + 15*Edel4*V2(nextState, nextAleph4state) 
                 + 20*Edel3*V3(nextState, nextAleph4state) 
                 + 15*Edel2*V4(nextState, nextAleph4state) 
                 + 6*Edel*V5(nextState, nextAleph4state) 
                 + V6(nextState, nextAleph4state); // ! recursion here !
        }
      }}));
      if (debug) console.log("| Q6", prettyState(state), action, aleph4action, q6);
      return q6;
    });
    // ! recursive function:
    var V6 = dp.cache(function(state, aleph4state){
      var locPol = localPolicy(state, aleph4state);
      var v6 = expectation(Infer({ model() {
        var actionAndAleph = sample(locPol);  // ! recursion here !
        return Q6(state, actionAndAleph[0], actionAndAleph[1]);  // ! recursion here !
      }}));
      if (debug) console.log("| V6", prettyState(state), aleph4state, v6);
      return v6;
    });

    // Expected powers of difference between total and some target value v,
    // needed for estimating moments of probabilistic policies in loss function,
    // where v will be an estimate of V(state):

    /* currently unused:
        var Qv = dp.cache(function(s, // state
                                a, // action
                                al, // aleph
                                v // target value
                ){
            return Q(s,a,al) - v;
        });
    */
    var relativeQ2 = dp.cache(function(s, a, al /*aleph4action*/, v){
        var res = Q2(s,a,al) 
                  - 2*Q(s,a,al)*v 
                  + squared(v);
        if (debug) console.log("| Q2v", prettyState(s), a, al, v, res);
        return res;
    });
    /*
        var relativeQ3 = dp.cache(function(s, a, al, v){
            return Q3(s,a,al) - 3*Q2(s,a,al)*v + 3*Q(s,a,al)*squared(v) - cubed(v);
        });
    */
    var relativeQ4 = dp.cache(function(s, a, al, v){
        return Q4(s,a,al) 
               - 4*Q3(s,a,al)*v 
               + 6*Q2(s,a,al)*squared(v) 
               - 4*Q(s,a,al)*cubed(v) 
               + Math.pow(v,4);
    });
    /*    
        var relativeQ5 = dp.cache(function(s, a, al, v){
            return Q5(s,a,al) - 5*Q4(s,a,al)*v + 10*Q3(s,a,al)*squared(v) 
                - 10*Q2(s,a,al)*cubed(v) + 5*Q(s,a,al)*Math.pow(v,4) - Math.pow(v,5);
        }); 
    */
    var relativeQ6 = dp.cache(function(s, a, al, v){
        return Q6(s,a,al) 
               - 6*Q5(s,a,al)*v 
               + 15*Q4(s,a,al)*squared(v)
               - 20*Q3(s,a,al)*cubed(v) 
               + 15*Q2(s,a,al)*Math.pow(v,4) 
               - 6*Q(s,a,al)*Math.pow(v,5) 
               + Math.pow(v,6);
    });
    
    // TODO: the following should maybe better be w.r.t. the initial aspiration interval, not the current state's:

    // loss based on a "cup" shaped potential centered at the mid-point of the aspiration interval
    // that is almost completely flat in the middle half of the interval 
    // (https://www.wolframalpha.com/input?i=plot+%28x-.5%29%5E6+from+0+to+1):

    var cupLoss_action = dp.cache(function(state, action, aleph4state, aleph4action){
      var res = relativeQ6(state, action, aleph4action, midpoint(aleph4state));
      if (debug) console.log("| cupLoss_action", prettyState(state), action, aleph4state, res);
      return res;
    });
    // recursive function:
    var cupLoss_state = dp.cache(function(state, unclippedAleph){
      var aleph4state = aspiration4state(state, unclippedAleph),
          locPol = localPolicy(state, aleph4state); // ! recursion here !
      return expectation(Infer({ model() {
        var actionAndAleph = sample(locPol);
        return cupLoss_action(state, actionAndAleph[0], aleph4state, actionAndAleph[1]);
      }
      }));
    });

    // Squared deviation of local relative aspiration (midpoint of interval) from 0.5:
    // recursive function:
    var LRAdev_action = dp.cache(function(state, action, aleph4action, myopic) {
      // Note for ANN approximation: LRAdev_action must be between 0 and 0.25 
      var Edel = expectedDelta(state, action),
          res = expectation(Infer({ model() {
            var localLRAdev = squared(0.5 - relativePosition(minAdmissibleQ(state, action), midpoint(aleph4action), maxAdmissibleQ(state, action)));
            if (state.terminateAfterAction || myopic){
              return localLRAdev;
            } else {
              var nextState = transition(state, action),
                  nextAleph4state = propagateAspiration(state, action, aleph4action, Edel, nextState);
              return localLRAdev + LRAdev_state(nextState, nextAleph4state); // ! recursion here !
            }
          }}));
      return res;
    });
    // recursive function:
    var LRAdev_state = dp.cache(function(state, aleph4state) {
      var locPol = localPolicy(state, aleph4state);
      return expectation(Infer({ model() { 
        var actionAndAleph = sample(locPol);  // ! recursion here !
        return LRAdev_action(state, actionAndAleph[0], actionAndAleph[1]); // ! recursion here !
      }}));
    });
  
    // TODO: verify the following two formulas for expected Delta variation along a trajectory:

    // Expected total of ones (= expected length of trajectory), for computing the expected Delta variation along a trajectory:
    // ! recursive function:
    var Q_ones = dp.cache(function(state, action, aleph4action){
      // Note for ANN approximation: Q_ones must be nonnegative. 
      var q_ones = expectation(Infer({ model() {
        var Edel = expectedDelta(state, action);
        if (state.terminateAfterAction || aleph4action === undefined){
          return 1;
        } else {
          var nextState = transition(state, action),
              nextAleph4state = propagateAspiration(state, action, aleph4action, Edel, nextState);
          return 1 + V_ones(nextState, nextAleph4state);  // ! recursion here !
        }
      }}));
      if (debug) console.log("| Q_ones", prettyState(state), action, aleph4action, q_ones);
      return q_ones;
    });
    // ! recursive function:
    var V_ones = dp.cache(function(state, aleph4state){
      var locPol = localPolicy(state, aleph4state);
      var v_ones = expectation(Infer({ model() { 
        var actionAndAleph = sample(locPol);  // ! recursion here !
        return Q_ones(state, actionAndAleph[0], actionAndAleph[1]);  // ! recursion here !
      }}));
      if (debug) console.log("| V_ones", prettyState(state), aleph4state, v_ones);
      return v_ones;
    });

    // Expected total of squared Deltas, for computing the expected Delta variation along a trajectory:
    // ! recursive function:
    var Q_DeltaSquare = dp.cache(function(state, action, aleph4action){
      // Note for ANN approximation: Q_DeltaSquare must be nonnegative. 
      var qDsq = expectation(Infer({ model() {
        var Edel = expectedDelta(state, action),
            EdelSq = squared(Edel) + varianceOfDelta(state, action);
        if (state.terminateAfterAction || aleph4action === undefined){
          return EdelSq;
        } else {
          var nextState = transition(state, action),
              nextAleph4state = propagateAspiration(state, action, aleph4action, Edel, nextState);
          return EdelSq + V_DeltaSquare(nextState, nextAleph4state);  // ! recursion here !
        }
      }}));
      if (debug) console.log("| Q_DeltaSquare", prettyState(state), action, aleph4action, qDsq);
      return qDsq;
    });
    // ! recursive function:
    var V_DeltaSquare = dp.cache(function(state, aleph4state){
      var locPol = localPolicy(state, aleph4state);
      var vDsq = expectation(Infer({ model() { 
        var actionAndAleph = sample(locPol);  // ! recursion here !
        return Q_DeltaSquare(state, actionAndAleph[0], actionAndAleph[1]);  // ! recursion here !
      }}));
      if (debug) console.log("| V_DeltaSquare", prettyState(state), aleph4state, vDsq);
      return vDsq;
    });
  
    // Other safety criteria:
  
    // Shannon entropy of behavior
    // (actually, negative KL divergence relative to uninformedPolicy (e.g., a uniform distribution),
    // to be consistent under action cloning or action refinement):
    // recursive function:
    var behaviorEntropy_action = dp.cache(function(state, actionProbability, action, aleph4action) {
      // Note for ANN approximation: behaviorEntropy_action must be <= 0 (!) 
      // because it is the negative (!) of a KL divergence. 
      var Edel = expectedDelta(state, action),
          res = expectation(Infer({ model() {
            var uninfPol = uninformedPolicy ? uninformedPolicy(state) : undefined,
                localEntropy = (uninfPol ? uninfPol.score(action) : 0) 
                    - Math.log(actionProbability)
                    + (internalActionEntropy ? internalActionEntropy(state, action) : 0);
            if (state.terminateAfterAction || aleph4action === undefined){
              return localEntropy;
            } else {
              var nextState = transition(state, action),
                  nextAleph4state = propagateAspiration(state, action, aleph4action, Edel, nextState);
              return localEntropy + behaviorEntropy_state(nextState, nextAleph4state); // ! recursion here !
            }
          }}));
      return res;
    });
    // recursive function:
    var behaviorEntropy_state = dp.cache(function(state, aleph4state) {
      var locPol = localPolicy(state, aleph4state);
      return expectation(Infer({ model() { 
        var actionAndAleph = sample(locPol);  // ! recursion here !
        return behaviorEntropy_action(state, Math.exp(locPol.score(actionAndAleph)), 
                                      actionAndAleph[0], actionAndAleph[1]); // ! recursion here !
      }}));
    });
  
    // KL divergence of behavior relative to refPolicy (or uninformedPolicy if refPolicy is undefined):
    // recursive function:
    var behaviorKLdiv_action = dp.cache(function(state, actionProbability, action, aleph4action) {
      // Note for ANN approximation: behaviorKLdiv_action must be nonnegative. 
      var refPol = referencePolicy ? referencePolicy(state) : uninformedPolicy ? uninformedPolicy(state) : undefined;
      if (refPol) {
        var Edel = expectedDelta(state, action),
            res = expectation(Infer({ model() {
            var localDivergence = Math.log(actionProbability) - refPol.score(action);
            if (state.terminateAfterAction || aleph4action === undefined){
              return localDivergence;
            } else {
              var nextState = transition(state, action),
                  nextAleph4state = propagateAspiration(state, action, aleph4action, Edel, nextState);
              return localDivergence + behaviorKLdiv_state(nextState, nextAleph4state); // ! recursion here !
            }
          }}));
        return res;
      } else {
        return undefined;
      }
    });
    // recursive function:
    var behaviorKLdiv_state = dp.cache(function(state, aleph4state) {
      var locPol = localPolicy(state, aleph4state);
      return expectation(Infer({ model() { 
        var actionAndAleph = sample(locPol);  // ! recursion here !
        return behaviorKLdiv_action(state, Math.exp(locPol.score(actionAndAleph)), actionAndAleph[0], actionAndAleph[1]); // ! recursion here !
      }}));
    });
  
    // other loss:
    // recursive function:
    var otherLoss_action = dp.cache(function(state, action, aleph4action) {
      var Edel = expectedDelta(state, action),
          loss = expectation(Infer({ model() {
            var localLoss = otherLocalLoss(state, action);
            if (state.terminateAfterAction || aleph4action === undefined){
              return localLoss;
            } else {
              var nextState = transition(state, action),
                  nextAleph4state = propagateAspiration(state, action, aleph4action, Edel, nextState);
              return localLoss + otherLoss_state(nextState, nextAleph4state); // ! recursion here !
            }
          }}));
      return loss;
    });
    // recursive function:
    var otherLoss_state = dp.cache(function(state, aleph4state) {
      var locPol = localPolicy(state, aleph4state); // ! recursion here !
      return expectation(Infer({ model() { 
        var actionAndAleph = sample(locPol);
        return otherLoss_action(state, actionAndAleph[0], actionAndAleph[1])  // ! recursion here !
      }}));
    });
  
    var randomTieBreaker = dp.cache(function(state, action) { return Math.random(); });
  

    // now we can combine all of the above quantities to a combined (safety) loss function:

    // ! recursive function:
    var combinedLoss = dp.cache(function(s /*state*/, a /*action*/, 
                                         al4s /*aleph4state*/, al4a /*aleph4action*/, 
                                         p /*estActionProbability*/) { 
      if (verbose || debug) console.log(pad(s),"| | combinedLoss, state",prettyState(s),"action",a,"aleph4state",al4s,"aleph4action",al4a,"estActionProbability",p,"...");

      // cheap criteria, including some myopic versions of the more expensive ones:
      var lRandom = lossCoeff4Random != 0 ? lossCoeff4Random * randomTieBreaker(s, a) : 0,
          lFeasibilityPower = lossCoeff4FeasibilityPower != 0 ? lossCoeff4FeasibilityPower * squared(maxAdmissibleQ(s, a) - minAdmissibleQ(s, a)) : 0,
          lMP = lossCoeff4MP != 0 ? lossCoeff4MP * messingPotential_action(s, a) : 0,
          lLRA1 = lossCoeff4LRA1 != 0 ? lossCoeff4LRA1 * LRAdev_action(s, a, al4a, true) : 0,
          lTime1 = lossCoeff4Time1 != 0 ? lossCoeff4Time1 * (s.terminateAfterAction ? 0 : 1) : 0,
          lEntropy1 = lossCoeff4Entropy1 != 0 ? lossCoeff4Entropy1 * behaviorEntropy_action(s, p, a, undefined) : 0,
          lKLdiv1 = lossCoeff4KLdiv1 != 0 ? lossCoeff4KLdiv1 * behaviorKLdiv_action(s, p, a, undefined) : 0;

      // moment-based criteria:
      // (To compute expected powers of deviation from V(s), we cannot use the actual V(s) 
      // because we don't know the local policy at s yet. Hence we use a simple estimate based on aleph4state)
      var estVs = midpoint(al4s),
          lVariance = lossCoeff4Variance != 0 ? lossCoeff4Variance * relativeQ2(s, a, al4a, estVs) : 0, // ! recursion here !
          lFourth = lossCoeff4Fourth != 0 ? lossCoeff4Fourth * relativeQ4(s, a, al4a, estVs) : 0, // ! recursion here ! 
          lCup = lossCoeff4Cup != 0 ? lossCoeff4Cup * cupLoss_action(s, a, al4s, al4a) : 0, // ! recursion here !
          lLRA = lossCoeff4LRA != 0 ? lossCoeff4LRA * LRAdev_action(s, a, al4a) : 0; // ! recursion here !

      // timing-related criteria:
      var q_ones = (lossCoeff4DeltaVariation != 0 || lossCoeff4Time != 0) ? Q_ones(s, a, al4a) : undefined,
          lTime = lossCoeff4Time != 0 ? lossCoeff4Time * q_ones : 0,
          lDeltaVariation = lossCoeff4DeltaVariation != 0 ? lossCoeff4DeltaVariation * (
              Q_DeltaSquare(s, a, al4a) / q_ones -  Q2(s, a, al4a) / squared(q_ones)
            ) : 0; // ! recursion here !

      // randomization-related criteria:
      var lEntropy = lossCoeff4Entropy != 0 ? lossCoeff4Entropy * behaviorEntropy_action(s, p, a, al4a) : 0, // ! recursion here !
          lKLdiv = lossCoeff4KLdiv != 0 ? lossCoeff4KLdiv * behaviorKLdiv_action(s, p, a, al4a) : 0; // ! recursion here !

      var lOther = otherLocalLoss && (lossCoeff4OtherLoss != 0) ? lossCoeff4otherLoss * otherLoss_action(s, a, al4a) : 0, // ! recursion here !
          res = lRandom + lFeasibilityPower + lMP + lLRA1 + lTime1 + lEntropy1 + lKLdiv1
                + lVariance + lFourth + lCup + lLRA 
                + lTime + lDeltaVariation 
                + lEntropy + lKLdiv 
                + lOther;
      if (verbose || debug) console.log(pad(s),"| | combinedLoss, state",prettyState(s),"action",a,"aleph4state",al4s,"aleph4action",al4a,"estActionProbability",p,":",res,"\n"+pad(s),"| |  ",JSON.stringify({
                lRandom, lFeasibilityPower, lMP, lLRA1, lTime1, lEntropy1, lKLdiv1,
                lVariance, lFourth, lCup, lLRA, 
                lTime, lDeltaVariation, 
                lEntropy, lKLdiv, 
                lOther}));
      return res;
    });

    var getData = function() {
      var stateActionPairs = Array.from(stateActionPairsSet),
          states = Array.from(_W.setFrom(map(function(pair) { return pair[0]; }, stateActionPairs))),
          locs = map(function(state) { return state.loc; }, states);
      return { states, stateActionPairs, locs };
    };

    return { 
      minAdmissibleQ, maxAdmissibleQ, minAdmissibleV, maxAdmissibleV,
      localPolicy, localPolicyData, propagateAspiration,
      Q, V, Q2, V2, Q_DeltaSquare, V_DeltaSquare, Q_ones, V_ones,
      messingPotential_state, messingPotential_action,
      behaviorEntropy_state, behaviorEntropy_action, 
      behaviorKLdiv_state, behaviorKLdiv_action, 
      cupLoss_state, cupLoss_action,
      otherLoss_state, otherLoss_action, 
      combinedLoss,
      getData
    };
};
