// make sure code runs in browser and on command line:
var inBrowser = typeof(argv) === 'undefined',
    _argv = inBrowser ? {} : argv;

// agent parameters:
var params = extend({
  }, _argv);

// simulation options:
var options = extend({
      aleph0: [1.2,1.4], // initial global aspiration (desired expected return);
      debug: false, // if true, print debug messages
    }, _argv),
    aleph0 = typeof(options.aleph0Lo) === 'undefined' ? options.aleph0 : [options.aleph0Lo, options.aleph0Hi];

// fold: Restaurant constants, tableToUtilityFunction

var ___ = ' ';
var G1 = { name : 'G1' };
var G2 = { name : 'G2' };
var G3 = { name : 'G3' };

var tableToIndicatorIncrementFct = function(table, feature) {
  return function(state, action) {
    var f = feature(state), stateFeatureName = f.name, inc = stateFeatureName ? table[stateFeatureName] : table[f['0']];
    return inc;
  };
};
// 

// Construct world 

var grid = [
  ['#', '#', '#', '#', '#'],
  ['#', '#',  G2, '#', '#'],
  ['#', G1 , ___,  G3, '#'],
  ['#', '#', '#', '#', '#']
];

var mdp = makeGridWorldMDP({
  grid,
  start: [2, 1],
  totalTime: 2
});

var world = mdp.world;
var transition = world.transition;
var stateToActions = world.stateToActions;


// Specify distribution of indicator increments (via expected value and variancs) 
// (Note: under a "maximization" paradigm, this would be called "reward" or "utility")

var expectedIndicatorIncrementTable = {
  'G1': 1,
  'G2': 2,
  'G3': 3,
  ' ': 0
};
var varianceOfIndicatorIncrementTable = {
  'G1': 0,
  'G2': 0,
  'G3': 0,
  ' ': 0 
};

var f0 = world.feature, feature = function(s) {return extend(f0(s), {test: "hello"})};

var expectedDelta = tableToIndicatorIncrementFct(expectedIndicatorIncrementTable, feature),
    varianceOfDelta = tableToIndicatorIncrementFct(varianceOfIndicatorIncrementTable, feature); 


// uninformedP policy for use in Shannon entropy: 
var uninformedPolicy = function(state) {
  return Categorical({vs: ["u", "d", "l", "r"], ps: [0.25, 0.25, 0.25, 0.25]});
}
// (Note: when refining an action a into variants a', a'', 
// the probabilities of a under uninformedPolicy should split into two parts for a' and a'' additively,
// since then behavior remains consistent. In particular, if an action is cloned and its uninformedPolicy 
// probability is split somehow, behavior should remain invariant.)

// Reference policy for use in KL divergence
// (can be used to steer agent towards certain actions):
var refPolicy = function(state) { 
  return Categorical({vs: ["u", "d", "l", "r"], ps: [0.7, 0.1, 0.1, 0.1]}); 
};

// initialize the agent
var agent = makeMDPAgentSatisfia(extend(params, {
    expectedIndicatorIncrement: expectedDelta, varianceOfIndicatorIncrement: varianceOfDelta, 
    uninformedPolicy, refPolicy,
    options
  }), world);

// extract its methods:
var getLocalPolicy = agent.getLocalPolicy, propagateAspiration = agent.propagateAspiration,
    getAspiration4state = agent.getAspiration4state, 
    G3 = agent.V, V2 = agent.V2,
    entropy = agent.entropy, KLdiv = agent.KLdiv;

// Generate and draw a trajectory:
var simulate = function(state, aleph) {
  if (options.debug) console.log("simulate", state, aleph);
  var localPolicy = getLocalPolicy(state, aleph),
      action = sample(localPolicy),
      r = expectedDelta(state, action),
      r2 = squared(r) + varianceOfDelta(state, action);
  console.log(localPolicy.params);
  var sa = [state, action];
  if (state.terminateAfterAction) {
    return { 
      trajectory: [sa], // sequence of [state, action] pairs
      conditionalExpectedIndicator: r, // expected indicator conditional on this trajectory
      conditionalExpectedSquaredIndicator: r2 // expected squared indicator conditional on this trajectory
    };
  } else {
    var nextState = transition(state, action),
        nextAleph = propagateAspiration(state, aleph, action, r, nextState),
        nextOut = simulate(nextState, nextAleph);
    return { 
      trajectory: [sa].concat(nextOut.trajectory), 
      conditionalExpectedIndicator: r + nextOut.conditionalExpectedIndicator,
      conditionalExpectedSquaredIndicator: r2 + 2*r*nextOut.conditionalExpectedIndicator + nextOut.conditionalExpectedSquaredIndicator
    };
  }
};


// calculate expected return and return std.dev. by using the inbuilt expectation function:

var expectedIndicator = expectation(Infer({ model() {
  return simulate(mdp.startState, aleph0).conditionalExpectedIndicator;
}}));
console.log("in expectation, we desired indicator", aleph0, "and actually get", expectedIndicator);

var expectedSquaredIndicator = expectation(Infer({ model() {
  return simulate(mdp.startState, aleph0).conditionalExpectedSquaredIndicator;
}})),
stddev = Math.sqrt(expectedSquaredIndicator - Math.pow(expectedIndicator,2));
console.log("indicator has a std.dev. of", stddev, 
            "which should equal", Math.sqrt(V2(mdp.startState, aleph0) - Math.pow(G3(mdp.startState, aleph0),2)));

console.log("Entropy of policy is", entropy(mdp.startState, aleph0));
console.log("KL divergence from reference policy is", KLdiv(mdp.startState, aleph0));

if (inBrowser) {
  // simulate and show a single trajectory:
  var out = simulate(mdp.startState, aleph0);
  viz.gridworld(world, { trajectory: map(first, out.trajectory) })
}
