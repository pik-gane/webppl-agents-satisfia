var env = getEnv(),
    argv = env.argv,
    params = extend({
      // some env-specific parameter settings
    }, argv),
    options = extend({
//      debug: true
      // some env-specific option settings
    }, argv),
    mdp = VerySimpleGW(argv.gw || "GW2", argv.gwparms, argv.time, argv.timeOutDelta),
    world = mdp.world,
    transition = world.transition,
    expectedDelta = mdp.expectedDelta,
    uninformedPolicy = mdp.uninformedPolicy,
    refPolicy = mdp.refPolicy,
    startState = mdp.startState,
    aleph0 = mdp.aleph0,
    agent = makeMDPAgentSatisfia(extend(params, {
      expectedDelta, uninformedPolicy, refPolicy, aleph0, options
    }), world),
    localPolicy = agent.localPolicy, 
    propagateAspiration = agent.propagateAspiration,
    getAspiration4state = agent.getAspiration4state, 
    V = agent.V, 
    V2 = agent.V2,
    entropy = agent.behaviorEntropy_state, 
    KLdiv = agent.behaviorKLdiv_state,
    cupLoss = agent.cupLoss_state;

// Generate and draw a trajectory:
var simulate = function(state, aleph, _t) {
  var t = _t ? _t : 0;
  if (options.debug) console.log("simulate", pretty(state), aleph, t);
  var localPolicy = localPolicy(state, aleph),
      action = sample(localPolicy),
      Edel = expectedDelta(state, action);
  var stepData = {state, aleph, action, Edel};
  if (state.terminateAfterAction) {
    return { 
      trajectory: [stepData], // sequence of [state, action] pairs
      conditionalExpectedIndicator: Edel // expected indicator conditional on this trajectory
    };
  } else {
    var nextState = transition(state, action),
        nextAleph = propagateAspiration(state, aleph, action, Edel, nextState),
        nextOut = simulate(nextState, nextAleph, t+1);
    return { 
      trajectory: [stepData].concat(nextOut.trajectory), 
      conditionalExpectedIndicator: Edel + nextOut.conditionalExpectedIndicator
    };
  }
};

// verify meeting of expectations:
console.log("V", V(startState, aleph0));
console.log("entropy", entropy(mdp.startState, aleph0));
console.log("KLdiv", KLdiv(mdp.startState, aleph0));
console.log("cupLoss", cupLoss(mdp.startState, aleph0));
var gd = agent.getData, agentData = gd();

// estimate distribution of trajectories:

var trajDist = Infer({ model() {
  return simulate(mdp.startState, aleph0).trajectory;
}}).getDist();
console.log("trajDist", trajDist);
var trajData = trajDist2TrajData(trajDist, agent);
//console.log("trajData", trajData);
var locActionData = webpplAgents.trajDist2LocActionData(trajDist, trajData);
console.log("locActionData", locActionData);

console.log("\nminFeasibleQ:");
console.log(stateActionFct2ASCII(agent.minFeasibleQ, agentData.stateActionPairs));
console.log("\nmaxFeasibleQ:");
console.log(stateActionFct2ASCII(agent.maxFeasibleQ, agentData.stateActionPairs));

console.log("\nQ:");
console.log(webpplAgents.locActionData2ASCII(locActionData.Q));

console.log("\ncupLoss:");
console.log(webpplAgents.locActionData2ASCII(locActionData.cupLoss));

console.log("\naction frequencies:");
console.log(webpplAgents.locActionData2ASCII(locActionData.actionFreq));



