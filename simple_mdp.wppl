// make sure code runs in browser and on command line:
var inBrowser = typeof(argv) === 'undefined',
    _argv = inBrowser ? {} : argv;

// agent parameters:
var params = extend({
    maxLambda: 0.99, // upper bound on local relative aspiration in each step (must be minLambda...1)
    minLambda: 0.01, // lower bound on local relative aspiration in each step (must be 0...maxLambda)
    lossCoeff4variance: 100.0, // weight of variance in loss function, must be >= 0
    lossCoeff4KLdiv: 0.0, // weight of KL divergence in loss function, must be >= 0
    lossCoeff4entropy: 1.0, // weight of entropy in loss function, must be >= 0
    lossCoeff4random: 0.0, // weight of random noise in loss function, must be >= 0
    onlyUseClosestActions: true, // if true, only use the two actions that are closest to the target aspiration in expectation
    softmaxInvTemp: 1.0, // inverse temperature of softmax mixture of actions, must be > 0
    rescalingDegree: 1.0, // degree (0...1) of aspiration rescaling. (expectation is only preserved if this is 1.0)
  }, _argv);

// simulation options:
var options = extend({
      aleph0: [1.2,1.4], // initial global aspiration (desired expected return);
      debug: false, // if true, print debug messages
    }, _argv),
    aleph0 = typeof(options.aleph0Lo) === 'undefined' ? options.aleph0 : [options.aleph0Lo, options.aleph0Hi];

// hand-coded world:

var startState = {name: '0', terminateAfterAction: false};
var stateToActions = function(state) {
  return ['pass'];
};
var transition = function(state, action) {
  return state == startState ? {name: '1', terminateAfterAction: true} : startState; 
}
var expectedIndicatorIncrement = function(state, action) {
  return state == startState ? 0 : 1;
};  
var varianceOfIndicatorIncrement = function(state, action) {
  return state == startState ? 0 : 1;
};  
var world = { stateToActions, transition }, mdp = { world, startState };
var refPolicy = function(state) { 
  return Categorical({vs: ['pass'], ps: [1.0]}); 
};
var uninformedPolicy = function(state) { 
  return Categorical({vs: ['pass'], ps: [1.0]}); 
};

// initialize the agent
var agent = makeMDPAgentSatisfia(extend(params, {
      expectedIndicatorIncrement, varianceOfIndicatorIncrement, 
      uninformedPolicy, refPolicy,
      options
    }), world);

// extract its methods:
var getLocalPolicy = agent.getLocalPolicy, propagateAspiration = agent.propagateAspiration,
    getAspiration4state = agent.getAspiration4state, 
    V = agent.V, V2 = agent.V2,
    entropy = agent.entropy, KLdiv = agent.KLdiv;

// Generate and draw a trajectory:
var simulate = function(state, aleph) {
  if (options.debug) console.log("simulate", state, aleph);
  var localPolicy = getLocalPolicy(state, aleph);
  var action = sample(localPolicy);
  var r = expectedIndicatorIncrement(state, action),
      r2 = Math.pow(r,2) + varianceOfIndicatorIncrement(state, action);
  var nextState = transition(state, action);
  var nextAleph = propagateAspiration(state, aleph, action, r, nextState);
  var sa = [state, action];
  if (state.terminateAfterAction) {
    return { 
      trajectory: [sa], // sequence of [state, action] pairs
      conditionalExpectedIndicator: r, // expected indicator conditional on this trajectory
      conditionalExpectedSquaredIndicator: r2 // expected squared indicator conditional on this trajectory
    };
  } else {
    var nextOut = simulate(nextState, nextAleph);
    return { 
      trajectory: [sa].concat(nextOut.trajectory), 
      conditionalExpectedIndicator: r + nextOut.conditionalExpectedIndicator,
      conditionalExpectedSquaredIndicator: r2 + 2*r*nextOut.conditionalExpectedIndicator + nextOut.conditionalExpectedSquaredIndicator
    };
  }
};


// calculate expected return and return std.dev. by using the inbuilt expectation function:

var expectedIndicator = expectation(Infer({ model() {
  return simulate(mdp.startState, aleph0).conditionalExpectedIndicator;
}}));
console.log("in expectation, we desired indicator", aleph0, "and actually get", expectedIndicator);

var expectedSquaredIndicator = expectation(Infer({ model() {
  return simulate(mdp.startState, aleph0).conditionalExpectedSquaredIndicator;
}})),
stddev = Math.sqrt(expectedSquaredIndicator - Math.pow(expectedIndicator,2));
console.log("indicator has a std.dev. of", stddev, 
            "which should equal", Math.sqrt(V2(mdp.startState, aleph0) - Math.pow(V(mdp.startState, aleph0),2)));

console.log("Entropy of policy is", entropy(mdp.startState, aleph0));
console.log("KL divergence from reference policy is", KLdiv(mdp.startState, aleph0));

// simulate and show a single trajectory:
var out = simulate(mdp.startState, aleph0);
console.log("trajectory", out.trajectory);

// Define the filename for the JSON file
var filename = 'output.json';

// Read existing JSON data from the file (if it exists)
var existingData = json.read(filename) || [];

// Assuming you have a single data object with keys and values
var data1 = {
  key1: 'expectedIndicator',
  value1: expectedIndicator,
  key2: 'aleph0',
  value2: aleph0,
  key3: 'maxLambda',
  value3: params.maxLambda,
  key4: 'minLambda',
  value4: params.minLambda
};

// Add new data to the existing data
existingData.push(data1);

// Write the updated data back to the JSON file
json.write(filename, existingData);
